{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load World Bank Indicator Metadata\n",
    "\n",
    "This notebook fetches all World Bank indicator metadata and writes each record to a Delta table incrementally. Supports resuming from where it left off.\n",
    "\n",
    "**Parameters (from DAB variables):**\n",
    "- `catalog`: Unity Catalog catalog name\n",
    "- `schema`: Schema name within the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wbgapi tqdm requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration from DAB variables (set via job parameters or widgets)\n",
    "# These are passed from databricks.yml when run as part of the workflow\n",
    "\n",
    "dbutils.widgets.text(\"catalog\", \"quant_risk_dev\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema\", \"indicators\", \"Schema Name\")\n",
    "dbutils.widgets.dropdown(\"fresh_start\", \"false\", [\"true\", \"false\"], \"Fresh Start\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema\")\n",
    "TABLE_NAME = \"worldbank_indicators\"\n",
    "FULL_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\"\n",
    "FRESH_START = dbutils.widgets.get(\"fresh_start\").lower() == \"true\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Catalog: {CATALOG}\")\n",
    "print(f\"  Schema: {SCHEMA}\")\n",
    "print(f\"  Table: {FULL_TABLE_NAME}\")\n",
    "print(f\"  Fresh Start: {FRESH_START}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wbgapi as wb\n",
    "import requests\n",
    "from requests.exceptions import RequestException, Timeout\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"indicator_id\", StringType(), False),\n",
    "    StructField(\"indicator_name\", StringType(), True),\n",
    "    StructField(\"long_definition\", StringType(), True),\n",
    "    StructField(\"source_organization\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"topics\", StringType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"periodicity\", StringType(), True),\n",
    "    StructField(\"aggregation_method\", StringType(), True),\n",
    "    StructField(\"license_type\", StringType(), True),\n",
    "    StructField(\"embedding_text\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_indicator_metadata(indicator_id: str) -> dict:\n",
    "    \"\"\"Fetch full metadata from World Bank API directly.\n",
    "    \n",
    "    Args:\n",
    "        indicator_id: World Bank indicator ID (e.g., 'SP.POP.TOTL')\n",
    "        \n",
    "    Returns:\n",
    "        Metadata dictionary from World Bank API\n",
    "        \n",
    "    Raises:\n",
    "        RequestException: If API request fails\n",
    "        ValueError: If response format is unexpected\n",
    "    \"\"\"\n",
    "    url = f\"https://api.worldbank.org/v2/indicator/{indicator_id}?format=json\"\n",
    "    \n",
    "    resp = requests.get(url, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    \n",
    "    if len(data) < 2 or not data[1]:\n",
    "        raise ValueError(f\"Unexpected API response format for {indicator_id}\")\n",
    "    \n",
    "    return data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fetch function\n",
    "test_meta = fetch_indicator_metadata(\"SP.POP.TOTL\")\n",
    "print(f\"Keys: {test_meta.keys()}\")\n",
    "print(f\"sourceNote: {test_meta.get('sourceNote', '')[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if table exists and get already loaded IDs\n",
    "existing_ids = set()\n",
    "\n",
    "if FRESH_START:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {FULL_TABLE_NAME}\")\n",
    "    spark.sql(f\"DROP VIEW IF EXISTS {FULL_TABLE_NAME}\")\n",
    "    print(\"Fresh start - dropped existing table\")\n",
    "else:\n",
    "    try:\n",
    "        existing_df = spark.sql(f\"SELECT indicator_id FROM {FULL_TABLE_NAME}\")\n",
    "        existing_ids = set(row.indicator_id for row in existing_df.collect())\n",
    "        print(f\"Resuming - found {len(existing_ids)} existing records\")\n",
    "    except AnalysisException as e:\n",
    "        # Table doesn't exist - this is expected on first run\n",
    "        if \"TABLE_OR_VIEW_NOT_FOUND\" in str(e) or \"does not exist\" in str(e).lower():\n",
    "            print(\"Table doesn't exist yet, starting fresh\")\n",
    "        else:\n",
    "            # Re-raise unexpected errors\n",
    "            raise\n",
    "\n",
    "# Create table if it doesn't exist\n",
    "if not existing_ids or FRESH_START:\n",
    "    # Ensure schema exists\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "    \n",
    "    empty_df = spark.createDataFrame([], schema)\n",
    "    empty_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(FULL_TABLE_NAME)\n",
    "    \n",
    "    # Set table description\n",
    "    spark.sql(f\"\"\"\n",
    "        COMMENT ON TABLE {FULL_TABLE_NAME} IS \n",
    "        'World Bank indicator metadata including economic, social, and environmental development indicators with embeddings for vector search.'\n",
    "    \"\"\")\n",
    "    print(f\"Created table {FULL_TABLE_NAME} with CDF enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all series IDs using wbgapi\n",
    "print(\"Fetching series list...\")\n",
    "series_list = wb.series.info()\n",
    "all_series_ids = [s.get(\"id\") for s in series_list.items]\n",
    "\n",
    "# Filter out already loaded IDs\n",
    "series_ids = [sid for sid in all_series_ids if sid not in existing_ids]\n",
    "print(f\"Total indicators: {len(all_series_ids)}\")\n",
    "print(f\"Already loaded: {len(existing_ids)}\")\n",
    "print(f\"Remaining to fetch: {len(series_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and append each record\n",
    "failed_ids = []\n",
    "\n",
    "for series_id in tqdm(series_ids, desc=\"Fetching metadata\"):\n",
    "    try:\n",
    "        meta = fetch_indicator_metadata(series_id)\n",
    "\n",
    "        indicator_id = meta.get(\"id\", series_id)\n",
    "        indicator_name = meta.get(\"name\", \"\") or \"\"\n",
    "        long_definition = meta.get(\"sourceNote\", \"\") or \"\"\n",
    "\n",
    "        topics_list = meta.get(\"topics\", []) or []\n",
    "        topics = \", \".join([t.get(\"value\", \"\") for t in topics_list if isinstance(t, dict)])\n",
    "\n",
    "        source_obj = meta.get(\"source\")\n",
    "        source = source_obj.get(\"value\", \"\") if isinstance(source_obj, dict) else \"\"\n",
    "\n",
    "        embedding_text = f\"{indicator_name}. {long_definition}\".strip()\n",
    "        if embedding_text == \".\":\n",
    "            embedding_text = indicator_name or indicator_id\n",
    "\n",
    "        record = [(\n",
    "            indicator_id,\n",
    "            indicator_name,\n",
    "            long_definition,\n",
    "            meta.get(\"sourceOrganization\", \"\") or \"\",\n",
    "            source,\n",
    "            topics,\n",
    "            meta.get(\"unit\", \"\") or \"\",\n",
    "            \"\",  # periodicity not in this API\n",
    "            \"\",  # aggregation_method not in this API\n",
    "            \"\",  # license_type not in this API\n",
    "            embedding_text,\n",
    "        )]\n",
    "\n",
    "        row_df = spark.createDataFrame(record, schema)\n",
    "        row_df.write.format(\"delta\").mode(\"append\").saveAsTable(FULL_TABLE_NAME)\n",
    "\n",
    "    except (RequestException, Timeout) as e:\n",
    "        # Network errors - log and track for retry\n",
    "        print(f\"Network error fetching {series_id}: {e}\")\n",
    "        failed_ids.append((series_id, str(e)))\n",
    "    except ValueError as e:\n",
    "        # Invalid response format - log and skip\n",
    "        print(f\"Invalid response for {series_id}: {e}\")\n",
    "        failed_ids.append((series_id, str(e)))\n",
    "\n",
    "print(f\"\\nDone! Failed: {len(failed_ids)} indicators\")\n",
    "if failed_ids:\n",
    "    print(\"Failed IDs (first 10):\")\n",
    "    for sid, err in failed_ids[:10]:\n",
    "        print(f\"  - {sid}: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "count = spark.sql(f\"SELECT COUNT(*) FROM {FULL_TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"Total records: {count}\")\n",
    "display(spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME} LIMIT 5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
