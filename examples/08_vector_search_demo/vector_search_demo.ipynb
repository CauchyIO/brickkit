{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Vector Search with BrickKit\n\nThis notebook demonstrates **end-to-end usage of BrickKit** for deploying a governed Vector Search solution.\n\n## What BrickKit Does\n\nBrickKit automates governance for Databricks resources:\n- **Naming conventions** - Environment-aware names (dev/acc/prd suffixes)\n- **Tagging** - Automatic cost center, team, compliance tags\n- **Ownership rules** - Enforce service principals for catalogs, groups for schemas\n- **Permission grants** - Ensure teams retain access after ownership changes\n- **Request for Access (RFA)** - Configure access request destinations with inheritance\n- **Validation** - Catch governance violations before deployment\n\n## BrickKit vs DAB (Databricks Asset Bundles)\n\n| Resource | DAB | BrickKit | Notes |\n|----------|-----|----------|-------|\n| **Catalog** | References only | Creates & governs | DAB passes variables, BrickKit deploys |\n| **Schema** | References only | Creates & governs | Same |\n| **Table** | References only | Creates & governs | BrickKit defines structure + tags |\n| **VS Endpoint** | Not supported | Creates & governs | DAB can't create these |\n| **VS Index** | Not supported | Creates & governs | DAB can't create these |\n| **Jobs/Workflows** | Defines | N/A | DAB's strength |\n| **Notebook sync** | Deploys | N/A | DAB syncs to workspace |\n\n**Key insight:** DAB deploys *code assets* (notebooks, jobs). BrickKit deploys *data assets* (catalogs, tables, VS).\n\n## What This Demo Shows\n\n1. Load a governance convention from YAML\n2. Define governed resources (Catalog, Schema, Table, VS Endpoint, VS Index)\n3. Deploy using BrickKit executors\n4. **Grant permissions** to team groups after ownership change\n5. Test vector search\n6. See what governance BrickKit applied automatically (tags, RFA, ownership, grants)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%pip install databricks-vectorsearch databricks-sdk pydantic pyyaml --quiet\ndbutils.library.restartPython()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === CONFIGURATION ===\n# Edit these widgets or override via DAB job parameters\n\ndbutils.widgets.text(\"catalog\", \"quant_risk\", \"Catalog Name (base)\")\ndbutils.widgets.text(\"schema\", \"indicators\", \"Schema Name\")\ndbutils.widgets.text(\"endpoint_name\", \"worldbank_vector_search\", \"VS Endpoint Name\")\ndbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"acc\", \"prd\"], \"Environment\")\ndbutils.widgets.dropdown(\"dry_run\", \"true\", [\"true\", \"false\"], \"Dry Run\")\n\n# Read widget values\nCATALOG_BASE = dbutils.widgets.get(\"catalog\")\nSCHEMA_NAME = dbutils.widgets.get(\"schema\")\nENDPOINT_NAME = dbutils.widgets.get(\"endpoint_name\")\nENVIRONMENT = dbutils.widgets.get(\"environment\")\nDRY_RUN = dbutils.widgets.get(\"dry_run\").lower() == \"true\"\n\n# Derived names\nTABLE_NAME = \"worldbank_indicators\"\nINDEX_NAME = f\"{TABLE_NAME}_index\"\n\nprint(f\"Environment: {ENVIRONMENT}\")\nprint(f\"Dry Run: {DRY_RUN}\")\nprint(f\"Catalog (base): {CATALOG_BASE}\")\nprint(f\"Schema: {SCHEMA_NAME}\")\nprint(f\"Endpoint: {ENDPOINT_NAME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === IMPORTS ===\nimport logging\nimport sys\nimport os\nfrom pathlib import Path\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.utils import AnalysisException\n\n# Add brickkit to path (not yet published to PyPI)\n# notebookPath() returns workspace-relative path, need /Workspace prefix for filesystem\nnotebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\nnotebook_dir = os.path.dirname(notebook_path)\n# Prepend /Workspace for filesystem access\nworkspace_notebook_dir = f\"/Workspace{notebook_dir}\"\nsrc_path = os.path.abspath(os.path.join(workspace_notebook_dir, \"..\", \"..\", \"src\"))\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\nprint(f\"Added to sys.path: {src_path}\")\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.vector_search.client import VectorSearchClient\n\n# BrickKit imports\nfrom brickkit import (\n    Catalog,\n    Schema,\n    Tag,\n    SecurableType,\n    VectorSearchEndpoint,\n    VectorSearchIndex,\n    load_convention,\n)\nfrom brickkit.models.tables import Table, ColumnInfo\nfrom brickkit.models.grants import Principal, AccessPolicy\nfrom brickkit.executors import (\n    CatalogExecutor,\n    SchemaExecutor,\n    GrantExecutor,\n    VectorSearchEndpointExecutor,\n    VectorSearchIndexExecutor,\n)\nfrom brickkit.models.base import set_current_environment\nfrom brickkit.models.enums import Environment\n\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# Set BrickKit environment\nENV_MAP = {\"dev\": Environment.DEV, \"acc\": Environment.ACC, \"prd\": Environment.PRD}\nset_current_environment(ENV_MAP[ENVIRONMENT])\n\nprint(f\"BrickKit environment set to: {ENVIRONMENT}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === LOAD GOVERNANCE CONVENTION ===\n# The convention defines naming patterns, required tags, and ownership rules\n\n# Use absolute path based on notebook location\nCONVENTION_PATH = os.path.join(workspace_notebook_dir, \"conventions\", \"financial_services.yml\")\nconvention = load_convention(CONVENTION_PATH)\n\nprint(f\"Loaded convention: {convention.name} (v{convention.version})\")\nprint(f\"Rules: {len(convention.schema.rules)}\")\nprint(f\"Default tags: {len(convention.schema.tags)}\")\n\n# Show what the convention enforces\nfor rule in convention.schema.rules:\n    mode = \"ENFORCED\" if rule.mode.value == \"enforced\" else \"ADVISORY\"\n    print(f\"  [{mode}] {rule.rule}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Sample Data\n",
    "\n",
    "We'll use a small inline dataset of World Bank indicators. This lets you run the full demo quickly without external API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAMPLE DATA ===\n",
    "# 20 World Bank indicators with embedding text for vector search\n",
    "\n",
    "SAMPLE_INDICATORS = [\n",
    "    (\"SP.POP.TOTL\", \"Population, total\", \"Total population counts all residents regardless of legal status or citizenship.\", \"Demographics\"),\n",
    "    (\"NY.GDP.MKTP.CD\", \"GDP (current US$)\", \"GDP at purchaser's prices is the sum of gross value added by all resident producers.\", \"Economy\"),\n",
    "    (\"NY.GDP.PCAP.CD\", \"GDP per capita (current US$)\", \"GDP per capita is gross domestic product divided by midyear population.\", \"Economy\"),\n",
    "    (\"SI.POV.DDAY\", \"Poverty headcount ratio at $2.15 a day\", \"Poverty headcount ratio at $2.15 a day is the percentage of the population living on less than $2.15 a day.\", \"Poverty\"),\n",
    "    (\"SI.POV.GINI\", \"Gini index\", \"Gini index measures the extent to which the distribution of income among individuals deviates from a perfectly equal distribution.\", \"Inequality\"),\n",
    "    (\"SL.UEM.TOTL.ZS\", \"Unemployment, total (% of labor force)\", \"Unemployment refers to the share of the labor force that is without work but available and seeking employment.\", \"Labor\"),\n",
    "    (\"FP.CPI.TOTL.ZG\", \"Inflation, consumer prices (annual %)\", \"Inflation as measured by the consumer price index reflects the annual percentage change in the cost of goods and services.\", \"Economy\"),\n",
    "    (\"SP.DYN.LE00.IN\", \"Life expectancy at birth, total (years)\", \"Life expectancy at birth indicates the number of years a newborn infant would live if patterns of mortality at birth were to stay the same.\", \"Health\"),\n",
    "    (\"SH.DYN.MORT\", \"Mortality rate, under-5 (per 1,000 live births)\", \"Under-five mortality rate is the probability per 1,000 that a newborn baby will die before reaching age five.\", \"Health\"),\n",
    "    (\"SE.ADT.LITR.ZS\", \"Literacy rate, adult total (% of people ages 15 and above)\", \"Adult literacy rate is the percentage of people ages 15 and above who can read and write a short simple statement.\", \"Education\"),\n",
    "    (\"SE.PRM.ENRR\", \"School enrollment, primary (% gross)\", \"Gross enrollment ratio is the ratio of total enrollment to the population of the age group that officially corresponds to the level of education.\", \"Education\"),\n",
    "    (\"EG.USE.ELEC.KH.PC\", \"Electric power consumption (kWh per capita)\", \"Electric power consumption measures the production of power plants and combined heat and power plants less transmission losses.\", \"Energy\"),\n",
    "    (\"EN.ATM.CO2E.PC\", \"CO2 emissions (metric tons per capita)\", \"Carbon dioxide emissions are those stemming from the burning of fossil fuels and the manufacture of cement.\", \"Environment\"),\n",
    "    (\"AG.LND.FRST.ZS\", \"Forest area (% of land area)\", \"Forest area is land under natural or planted stands of trees of at least 5 meters in situ.\", \"Environment\"),\n",
    "    (\"SH.XPD.CHEX.PC.CD\", \"Current health expenditure per capita (current US$)\", \"Current expenditures on health per capita in current US dollars.\", \"Health\"),\n",
    "    (\"IT.NET.USER.ZS\", \"Individuals using the Internet (% of population)\", \"Internet users are individuals who have used the Internet in the last 3 months.\", \"Technology\"),\n",
    "    (\"BX.KLT.DINV.CD.WD\", \"Foreign direct investment, net inflows (BoP, current US$)\", \"Foreign direct investment are the net inflows of investment to acquire a lasting management interest.\", \"Economy\"),\n",
    "    (\"GC.DOD.TOTL.GD.ZS\", \"Central government debt, total (% of GDP)\", \"Debt is the entire stock of direct government fixed-term contractual obligations to others outstanding.\", \"Economy\"),\n",
    "    (\"NE.EXP.GNFS.ZS\", \"Exports of goods and services (% of GDP)\", \"Exports of goods and services represent the value of all goods and other market services provided to the rest of the world.\", \"Trade\"),\n",
    "    (\"NE.IMP.GNFS.ZS\", \"Imports of goods and services (% of GDP)\", \"Imports of goods and services represent the value of all goods and other market services received from the rest of the world.\", \"Trade\"),\n",
    "]\n",
    "\n",
    "# Schema for the indicators table\n",
    "INDICATORS_SCHEMA = StructType([\n",
    "    StructField(\"indicator_id\", StringType(), False),\n",
    "    StructField(\"indicator_name\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"topic\", StringType(), True),\n",
    "    StructField(\"embedding_text\", StringType(), True),\n",
    "])\n",
    "\n",
    "def create_sample_dataframe(spark: SparkSession):\n",
    "    \"\"\"Create DataFrame from sample indicators with embedding text.\"\"\"\n",
    "    rows = [\n",
    "        (ind_id, name, desc, topic, f\"{name}. {desc}\")\n",
    "        for ind_id, name, desc, topic in SAMPLE_INDICATORS\n",
    "    ]\n",
    "    return spark.createDataFrame(rows, INDICATORS_SCHEMA)\n",
    "\n",
    "# Preview the sample data\n",
    "sample_df = create_sample_dataframe(spark)\n",
    "print(f\"Sample data: {sample_df.count()} indicators\")\n",
    "sample_df.select(\"indicator_id\", \"indicator_name\", \"topic\").show(5, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Fetch Real Data from World Bank API\n",
    "\n",
    "Uncomment and run the cell below to fetch real indicator metadata. This takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTIONAL: FETCH FROM WORLD BANK API ===\n",
    "# Uncomment this cell to fetch real data (takes several minutes)\n",
    "\n",
    "# %pip install wbgapi requests tqdm --quiet\n",
    "\n",
    "# import wbgapi as wb\n",
    "# import requests\n",
    "# from requests.exceptions import RequestException, Timeout\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def fetch_worldbank_indicators(spark: SparkSession, limit: int = 100):\n",
    "#     \"\"\"Fetch indicator metadata from World Bank API.\"\"\"\n",
    "#     series_list = wb.series.info()\n",
    "#     series_ids = [s.get(\"id\") for s in series_list.items][:limit]\n",
    "#     \n",
    "#     rows = []\n",
    "#     for series_id in tqdm(series_ids, desc=\"Fetching\"):\n",
    "#         try:\n",
    "#             url = f\"https://api.worldbank.org/v2/indicator/{series_id}?format=json\"\n",
    "#             resp = requests.get(url, timeout=30)\n",
    "#             resp.raise_for_status()\n",
    "#             data = resp.json()\n",
    "#             if len(data) >= 2 and data[1]:\n",
    "#                 meta = data[1][0]\n",
    "#                 name = meta.get(\"name\", \"\") or \"\"\n",
    "#                 desc = meta.get(\"sourceNote\", \"\") or \"\"\n",
    "#                 topics = meta.get(\"topics\", []) or []\n",
    "#                 topic = topics[0].get(\"value\", \"\") if topics else \"\"\n",
    "#                 embedding_text = f\"{name}. {desc}\".strip()\n",
    "#                 rows.append((series_id, name, desc, topic, embedding_text))\n",
    "#         except (RequestException, Timeout, ValueError) as e:\n",
    "#             print(f\"Skipping {series_id}: {e}\")\n",
    "#     \n",
    "#     return spark.createDataFrame(rows, INDICATORS_SCHEMA)\n",
    "\n",
    "# # Fetch real data (uncomment to use)\n",
    "# sample_df = fetch_worldbank_indicators(spark, limit=500)\n",
    "# print(f\"Fetched {sample_df.count()} indicators from World Bank API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Governed Resources\n",
    "\n",
    "Now we define our resources using BrickKit models. The convention automatically applies:\n",
    "- Environment-specific naming (e.g., `quant_risk_dev`)\n",
    "- Required governance tags\n",
    "- Ownership rules validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEFINE GOVERNED RESOURCES ===\n",
    "\n",
    "environment = ENV_MAP[ENVIRONMENT]\n",
    "\n",
    "# Get owners from convention (enforces SP for catalogs, Group for schemas)\n",
    "catalog_owner = convention.get_catalog_owner(environment)\n",
    "schema_owner = convention.get_owner(SecurableType.SCHEMA, environment)\n",
    "\n",
    "print(f\"Catalog owner: {catalog_owner.resolved_name} ({catalog_owner.principal_type.value})\")\n",
    "print(f\"Schema owner: {schema_owner.resolved_name} ({schema_owner.principal_type.value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CATALOG ===\n",
    "catalog_name = convention.generate_name(SecurableType.CATALOG, environment)\n",
    "\n",
    "catalog = Catalog(\n",
    "    name=catalog_name,\n",
    "    owner=catalog_owner,\n",
    "    comment=\"Risk Analytics catalog for quantitative trading\",\n",
    ")\n",
    "\n",
    "# Apply convention (adds tags, validates rules)\n",
    "convention.apply_to(catalog, environment)\n",
    "errors = convention.get_validation_errors(catalog)\n",
    "if errors:\n",
    "    raise ValueError(f\"Catalog validation failed: {errors}\")\n",
    "\n",
    "print(f\"Catalog: {catalog.name}\")\n",
    "print(f\"  Tags: {len(catalog.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SCHEMA ===\n",
    "schema = Schema(\n",
    "    name=SCHEMA_NAME,\n",
    "    catalog_name=catalog.name,\n",
    "    owner=schema_owner,\n",
    "    comment=\"World Bank indicator metadata for vector search\",\n",
    ")\n",
    "\n",
    "convention.apply_to(schema, environment)\n",
    "errors = convention.get_validation_errors(schema)\n",
    "if errors:\n",
    "    raise ValueError(f\"Schema validation failed: {errors}\")\n",
    "\n",
    "print(f\"Schema: {schema.fqdn}\")\n",
    "print(f\"  Tags: {len(schema.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# === TABLE ===\n# Define the table structure with BrickKit (not just raw PySpark write)\n\ntable = Table(\n    name=TABLE_NAME,\n    catalog_name=catalog.name,\n    schema_name=schema.name,\n    owner=schema_owner,\n    comment=\"World Bank indicator metadata with embeddings for semantic search\",\n    columns=[\n        ColumnInfo(name=\"indicator_id\", type=\"STRING\", nullable=False, comment=\"World Bank indicator code\"),\n        ColumnInfo(name=\"indicator_name\", type=\"STRING\", nullable=True, comment=\"Human-readable indicator name\"),\n        ColumnInfo(name=\"description\", type=\"STRING\", nullable=True, comment=\"Full description of the indicator\"),\n        ColumnInfo(name=\"topic\", type=\"STRING\", nullable=True, comment=\"Category/topic of the indicator\"),\n        ColumnInfo(name=\"embedding_text\", type=\"STRING\", nullable=True, comment=\"Text used for embedding generation\"),\n    ],\n    tags=[\n        Tag(key=\"data_source\", value=\"worldbank_api\"),\n        Tag(key=\"refresh_frequency\", value=\"weekly\"),\n        Tag(key=\"contains_pii\", value=\"false\"),\n    ],\n)\n\nconvention.apply_to(table, environment)\nerrors = convention.get_validation_errors(table)\nif errors:\n    raise ValueError(f\"Table validation failed: {errors}\")\n\nprint(f\"Table: {table.fqdn}\")\nprint(f\"  Columns: {len(table.columns)}\")\nprint(f\"  Tags: {len(table.tags)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VECTOR SEARCH ENDPOINT ===\n",
    "vs_endpoint = VectorSearchEndpoint(\n",
    "    name=ENDPOINT_NAME,\n",
    "    comment=\"Semantic search endpoint for risk analytics indicators\",\n",
    "    tags=[\n",
    "        Tag(key=\"purpose\", value=\"semantic_search\"),\n",
    "        Tag(key=\"model\", value=\"databricks-bge-large-en\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "convention.apply_to(vs_endpoint, environment)\n",
    "errors = convention.get_validation_errors(vs_endpoint)\n",
    "if errors:\n",
    "    raise ValueError(f\"Endpoint validation failed: {errors}\")\n",
    "\n",
    "print(f\"Endpoint: {vs_endpoint.resolved_name}\")\n",
    "print(f\"  Tags: {len(vs_endpoint.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === VECTOR SEARCH INDEX ===\n# Use table.fqdn to reference the governed table\n\nvs_index = VectorSearchIndex(\n    name=INDEX_NAME,\n    endpoint_name=ENDPOINT_NAME,\n    source_table=table.fqdn,  # Reference the governed Table model\n    primary_key=\"indicator_id\",\n    embedding_column=\"embedding_text\",\n    embedding_model=\"databricks-bge-large-en\",\n    pipeline_type=\"TRIGGERED\",\n    tags=[\n        Tag(key=\"index_type\", value=\"managed_embedding\"),\n    ],\n)\n\nconvention.apply_to(vs_index, environment)\nerrors = convention.get_validation_errors(vs_index)\nif errors:\n    raise ValueError(f\"Index validation failed: {errors}\")\n\nprint(f\"Index: {vs_index.resolved_name}\")\nprint(f\"  Source: {vs_index.source_table}\")\nprint(f\"  Endpoint: {vs_index.resolved_endpoint_name}\")\nprint(f\"  Tags: {len(vs_index.tags)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Deploy with BrickKit Executors\n\nBrickKit executors handle:\n- Idempotent create (skip if exists)\n- Wait for provisioning\n- Tag application\n- **Permission grants** - Ensure teams have access after ownership change\n- Error handling"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === INITIALIZE CLIENTS AND EXECUTORS ===\n\nws_client = WorkspaceClient()\nvs_client = VectorSearchClient()\n\ncatalog_executor = CatalogExecutor(ws_client, dry_run=DRY_RUN)\nschema_executor = SchemaExecutor(ws_client, dry_run=DRY_RUN)\ngrant_executor = GrantExecutor(ws_client, dry_run=DRY_RUN)\nendpoint_executor = VectorSearchEndpointExecutor(ws_client, dry_run=DRY_RUN)\nindex_executor = VectorSearchIndexExecutor(ws_client, dry_run=DRY_RUN)\n\nprint(f\"Executors initialized (dry_run={DRY_RUN})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY CATALOG ===\n",
    "result = catalog_executor.create(catalog)\n",
    "print(f\"Catalog: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY SCHEMA ===\n",
    "result = schema_executor.create(schema)\n",
    "print(f\"Schema: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# === APPLY GRANTS ===\n# After ownership change, the deploying user loses access. Grant permissions to the team group.\n# This ensures the schema owner group can access the catalog and work with the data.\n\n# Grant the schema owner group access to the catalog\n# The schema owner (grp_quant_team) needs USE_CATALOG to access schemas inside\ncatalog.grant(schema_owner, AccessPolicy.WRITER())\n\n# Apply all accumulated grants to Databricks\nresults = grant_executor.apply_privileges(catalog.privileges)\nfor result in results:\n    print(f\"Grants on {result.resource_name}: {result.operation.value} - {result.message}\")\n\n# Show what was granted\nprint(f\"\\nPrivileges granted on catalog:\")\nfor priv in catalog.privileges:\n    print(f\"  - {priv.principal}: {priv.privilege.value}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === WRITE DATA TO TABLE ===\n# Using the governed Table model's fqdn\n\nif not DRY_RUN:\n    # Write with Delta format and Change Data Feed enabled\n    (\n        sample_df\n        .write\n        .format(\"delta\")\n        .option(\"delta.enableChangeDataFeed\", \"true\")\n        .mode(\"overwrite\")\n        .saveAsTable(table.fqdn)\n    )\n    \n    # Verify\n    count = spark.table(table.fqdn).count()\n    print(f\"Table: {table.fqdn} - {count} rows written\")\nelse:\n    print(f\"[DRY RUN] Would write {sample_df.count()} rows to {table.fqdn}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY VECTOR SEARCH ENDPOINT ===\n",
    "result = endpoint_executor.create(vs_endpoint)\n",
    "print(f\"Endpoint: {result.operation.value} - {result.message}\")\n",
    "\n",
    "# Wait for endpoint to be online (uses executor's built-in wait logic)\n",
    "if not DRY_RUN and result.operation.value == \"CREATE\":\n",
    "    print(\"Waiting for endpoint to be online...\")\n",
    "    if endpoint_executor.wait_for_endpoint(vs_endpoint):\n",
    "        print(f\"Endpoint {vs_endpoint.resolved_name} is ONLINE\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Endpoint {vs_endpoint.resolved_name} failed to provision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY VECTOR SEARCH INDEX ===\n",
    "result = index_executor.create(vs_index)\n",
    "print(f\"Index: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test Vector Search\n",
    "\n",
    "The index syncs asynchronously. Once ready, we can run similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECK INDEX STATUS ===\n",
    "\n",
    "if not DRY_RUN:\n",
    "    FULL_INDEX_NAME = f\"{catalog.name}.{schema.name}.{vs_index.resolved_name}\"\n",
    "    \n",
    "    index = vs_client.get_index(\n",
    "        endpoint_name=vs_endpoint.resolved_name,\n",
    "        index_name=FULL_INDEX_NAME,\n",
    "    )\n",
    "    status = index.describe().get(\"status\", {})\n",
    "    print(f\"Index status: ready={status.get('ready', 'UNKNOWN')}\")\n",
    "    print(f\"Message: {status.get('message', 'N/A')}\")\n",
    "else:\n",
    "    print(\"[DRY RUN] Would check index status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN SIMILARITY SEARCH ===\n",
    "\n",
    "if not DRY_RUN:\n",
    "    TEST_QUERY = \"poverty and inequality measures\"\n",
    "    \n",
    "    try:\n",
    "        results = index.similarity_search(\n",
    "            query_text=TEST_QUERY,\n",
    "            columns=[\"indicator_id\", \"indicator_name\", \"description\", \"topic\"],\n",
    "            num_results=5,\n",
    "        )\n",
    "        \n",
    "        print(f\"Search: '{TEST_QUERY}'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        data = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "        for i, row in enumerate(data, 1):\n",
    "            print(f\"{i}. [{row[3]}] {row[1]}\")\n",
    "            print(f\"   {row[2][:80]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        if \"not ready\" in str(e).lower() or \"syncing\" in str(e).lower():\n",
    "            print(\"Index is still syncing. Please wait and try again.\")\n",
    "        else:\n",
    "            raise\n",
    "else:\n",
    "    print(\"[DRY RUN] Would run similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. What BrickKit Added (Governance Value)\n",
    "\n",
    "Let's see what governance BrickKit applied automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === GOVERNANCE SUMMARY ===\n\ndef display_resource_governance(name: str, resource):\n    \"\"\"Display governance metadata for a resource.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"{name}\")\n    print(f\"{'='*60}\")\n    \n    # Name (with environment suffix)\n    if hasattr(resource, 'resolved_name'):\n        print(f\"Name: {resource.resolved_name}\")\n    elif hasattr(resource, 'fqdn'):\n        try:\n            print(f\"Name: {resource.fqdn}\")\n        except ValueError:\n            print(f\"Name: {resource.name}\")\n    else:\n        print(f\"Name: {resource.name}\")\n    \n    # Owner\n    if hasattr(resource, 'owner') and resource.owner:\n        owner = resource.owner\n        print(f\"Owner: {owner.resolved_name} ({owner.principal_type.value})\")\n    \n    # Privileges (grants)\n    if hasattr(resource, 'privileges') and resource.privileges:\n        print(f\"Privileges ({len(resource.privileges)}):\")\n        for priv in resource.privileges:\n            print(f\"  - {priv.principal}: {priv.privilege.value}\")\n    \n    # Request for Access (RFA)\n    if hasattr(resource, 'request_for_access') and resource.request_for_access:\n        rfa = resource.request_for_access\n        print(f\"RFA Destination: {rfa.destination}\")\n        if rfa.instructions:\n            print(f\"RFA Instructions: {rfa.instructions}\")\n    \n    # Columns (for tables)\n    if hasattr(resource, 'columns') and resource.columns:\n        print(f\"Columns: {len(resource.columns)}\")\n    \n    # Tags\n    if hasattr(resource, 'tags') and resource.tags:\n        print(f\"Tags ({len(resource.tags)}):\")\n        for tag in sorted(resource.tags, key=lambda t: t.key):\n            print(f\"  - {tag.key}: {tag.value}\")\n\n# Display governance for all resources\ndisplay_resource_governance(\"CATALOG\", catalog)\ndisplay_resource_governance(\"SCHEMA\", schema)\ndisplay_resource_governance(\"TABLE\", table)\ndisplay_resource_governance(\"VECTOR SEARCH ENDPOINT\", vs_endpoint)\ndisplay_resource_governance(\"VECTOR SEARCH INDEX\", vs_index)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONVENTION RULES APPLIED ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONVENTION RULES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Convention: {convention.name} (v{convention.version})\")\n",
    "print()\n",
    "\n",
    "for rule in convention.schema.rules:\n",
    "    mode = \"ENFORCED\" if rule.mode.value == \"enforced\" else \"ADVISORY\"\n",
    "    print(f\"[{mode}] {rule.rule}\")\n",
    "    \n",
    "print()\n",
    "print(\"What this means:\")\n",
    "print(\"- Catalogs MUST be owned by service principals (not users)\")\n",
    "print(\"- All resources MUST be owned by SP or Group (no individual users)\")\n",
    "print(\"- Resources SHOULD have cost_center and team tags\")\n",
    "print(\"- BrickKit validated all these rules before deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === WHAT YOU DIDN'T HAVE TO DO ===\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"WHAT BRICKKIT DID FOR YOU\")\nprint(\"=\" * 60)\n\nbenefits = [\n    (\"Environment suffixes\", f\"All names automatically suffixed with '_{ENVIRONMENT}'\"),\n    (\"Governance tags\", f\"{len(catalog.tags)} tags auto-applied from convention\"),\n    (\"Ownership validation\", \"Verified catalog has SP owner, schema has Group owner\"),\n    (\"Permission grants\", f\"Granted {len(catalog.privileges)} privileges to team group after ownership change\"),\n    (\"Request for Access\", \"RFA configured with inheritance (table inherits from schema)\"),\n    (\"Idempotent deployment\", \"Executors skip if resource exists, sync tags if needed\"),\n    (\"Wait logic\", \"Built-in endpoint provisioning wait with timeout/retry\"),\n    (\"Consistent patterns\", \"Same governance across Catalog, Schema, Endpoint, Index\"),\n]\n\nfor benefit, detail in benefits:\n    print(f\"\\n{benefit}:\")\n    print(f\"  {detail}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Without BrickKit, you would manually:\")\nprint(\"  - Add environment suffixes to every resource name\")\nprint(\"  - Remember which tags to apply (and apply them consistently)\")\nprint(\"  - Validate ownership rules before deployment\")\nprint(\"  - Grant permissions to teams after changing ownership\")\nprint(\"  - Configure RFA on each securable individually\")\nprint(\"  - Write wait/retry logic for endpoint provisioning\")\nprint(\"  - Handle idempotency (check exists, update tags, etc.)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\nThis demo showed:\n\n1. **Convention Loading** - Governance rules from YAML\n2. **Governed Models** - `Catalog`, `Schema`, `Table`, `VectorSearchEndpoint`, `VectorSearchIndex`\n3. **Executors** - Idempotent deployment with built-in wait logic\n4. **Automatic Governance** - Tags, naming, ownership validation\n\n### BrickKit vs DAB Recap\n\n- **DAB** handles: notebook sync, job definitions, workflow orchestration\n- **BrickKit** handles: catalog/schema/table creation, VS endpoint/index, tags, validation\n- **Together**: DAB runs this notebook as a job, BrickKit deploys the governed resources\n\n### Next Steps\n\n- Modify `conventions/financial_services.yml` to change governance rules\n- Set `dry_run=false` to deploy for real\n- Try different environments (`dev`, `acc`, `prd`) to see naming changes\n- Add your own data source instead of sample indicators"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}