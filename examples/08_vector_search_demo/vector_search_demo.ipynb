{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search with BrickKit\n",
    "\n",
    "This notebook demonstrates **end-to-end usage of BrickKit** for deploying a governed Vector Search solution.\n",
    "\n",
    "## What BrickKit Does\n",
    "\n",
    "BrickKit automates governance for Databricks resources:\n",
    "- **Team management** - Define teams with workspaces and principals, auto-configure catalog bindings\n",
    "- **Principal management** - Define service principals for ownership (Unity Catalog compatible)\n",
    "- **Naming conventions** - Environment-aware names (dev/acc/prd suffixes)\n",
    "- **Tagging** - Automatic cost center, team, compliance tags\n",
    "- **Ownership rules** - Enforce service principals for all securables\n",
    "- **Permission grants** - Ensure teams retain access after ownership changes\n",
    "- **Request for Access (RFA)** - Configure access request destinations with inheritance\n",
    "- **Validation** - Catch governance violations before deployment\n",
    "\n",
    "## BrickKit vs DAB (Databricks Asset Bundles)\n",
    "\n",
    "| Resource | DAB | BrickKit | Notes |\n",
    "|----------|-----|----------|-------|\n",
    "| **Teams** | Not supported | Defines & manages | BrickKit organizes workspace + principals |\n",
    "| **SPNs** | Not supported | Defines & manages | BrickKit defines principals declaratively |\n",
    "| **Catalog** | References only | Creates & governs | DAB passes variables, BrickKit deploys |\n",
    "| **Schema** | References only | Creates & governs | Same |\n",
    "| **Table** | References only | Creates & governs | BrickKit defines structure + tags |\n",
    "| **VS Endpoint** | Not supported | Creates & governs | DAB can't create these |\n",
    "| **VS Index** | Not supported | Creates & governs | DAB can't create these |\n",
    "| **Jobs/Workflows** | Defines | N/A | DAB's strength |\n",
    "| **Notebook sync** | Deploys | N/A | DAB syncs to workspace |\n",
    "\n",
    "**Key insight:** DAB deploys *code assets* (notebooks, jobs). BrickKit deploys *data assets* (catalogs, tables, VS) and *principals* (teams, SPNs).\n",
    "\n",
    "## What This Demo Shows\n",
    "\n",
    "1. Load a governance convention from YAML\n",
    "2. **Define Team** with workspace and principals\n",
    "3. Define governed resources (Catalog, Schema, Table, VS Endpoint, VS Index)\n",
    "4. **Auto-configure workspace bindings** via `team.add_catalog()`\n",
    "5. Deploy using BrickKit executors\n",
    "6. **Grant permissions** to team groups after ownership change\n",
    "7. Test vector search\n",
    "8. See what governance BrickKit applied automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration\n",
    "\n",
    "Edit the constants below to customize the deployment. This notebook runs on both Databricks and locally:\n",
    "\n",
    "- **Databricks**: Full deployment including table writes and vector search\n",
    "- **Local**: Model definitions, validation, and executor calls (table writes skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade databricks-vectorsearch databricks-sdk pydantic pyyaml --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === CONFIGURATION ===\nCATALOG_BASE = \"quant_risk\"\nSCHEMA_NAME = \"indicators\"\nENDPOINT_NAME = \"worldbank_vector_search\"\nMANAGED_LOCATION = None  # Set if using Default Storage workspaces\nENVIRONMENT = \"dev\"  # \"dev\", \"acc\", or \"prd\"\nDRY_RUN = False\n\n# Workspace config (for workspace bindings)\nWORKSPACE_ID = \"4188055811360976\"  # Your workspace ID\nWORKSPACE_NAME = \"free-edition-workspace\"\n\n# Local execution config (ignored on Databricks)\n# Set this to your databricks CLI profile name that has valid auth\nDATABRICKS_PROFILE = \"adb-582244602844614\"\n\n# Derived names\nTABLE_NAME = \"worldbank_indicators\"\nINDEX_NAME = f\"{TABLE_NAME}_index\"\n\nprint(f\"Environment: {ENVIRONMENT}\")\nprint(f\"Dry Run: {DRY_RUN}\")\nprint(f\"Catalog (base): {CATALOG_BASE}\")\nprint(f\"Schema: {SCHEMA_NAME}\")\nprint(f\"Endpoint: {ENDPOINT_NAME}\")\nprint(f\"Workspace: {WORKSPACE_NAME} (ID: {WORKSPACE_ID})\")\nif MANAGED_LOCATION:\n    print(f\"Managed Location: {MANAGED_LOCATION}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === IMPORTS ===\nimport logging\nimport sys\nimport os\nfrom pathlib import Path\nfrom pyspark.sql.types import StructType, StructField, StringType\n\n# Detect environment\nIS_DATABRICKS = \"DATABRICKS_RUNTIME_VERSION\" in os.environ\n\n# Add brickkit to path (not yet published to PyPI)\nif IS_DATABRICKS:\n    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n    notebook_dir = os.path.dirname(notebook_path)\n    workspace_notebook_dir = f\"/Workspace{notebook_dir}\"\n    src_path = os.path.abspath(os.path.join(workspace_notebook_dir, \"..\", \"..\", \"src\"))\nelse:\n    # Local: use file path relative to this notebook\n    notebook_dir = Path(__file__).parent if \"__file__\" in dir() else Path.cwd()\n    src_path = str(notebook_dir.parent.parent / \"src\")\n    workspace_notebook_dir = str(notebook_dir)  # For convention path\n\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\nprint(f\"Added to sys.path: {src_path}\")\n\n# Create SparkSession - different approach for local vs Databricks\nif IS_DATABRICKS:\n    # On Databricks, spark is already available\n    from pyspark.sql import SparkSession\n    print(f\"Using Databricks SparkSession\")\nelse:\n    # Local: use databricks-connect with serverless\n    from databricks.connect import DatabricksSession\n    spark = DatabricksSession.builder.profile(DATABRICKS_PROFILE).serverless(True).getOrCreate()\n    print(f\"Connected to Databricks via databricks-connect (serverless)\")\n    print(f\"  Profile: {DATABRICKS_PROFILE}\")\n\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.vector_search.client import VectorSearchClient\n\n# BrickKit imports\nfrom brickkit import (\n    Catalog,\n    Schema,\n    Tag,\n    SecurableType,\n    VectorSearchEndpoint,\n    VectorSearchIndex,\n    load_convention,\n)\nfrom brickkit.models.tables import Table, ColumnInfo\nfrom brickkit.models.grants import Principal, AccessPolicy\nfrom brickkit.models.principals import ManagedGroup, ManagedServicePrincipal\nfrom brickkit.models.enums import PrincipalType, IsolationMode\nfrom brickkit.models.workspace_bindings import Workspace, WorkspaceRegistry\nfrom brickkit.models.teams import Team\nfrom brickkit.executors import (\n    CatalogExecutor,\n    SchemaExecutor,\n    GrantExecutor,\n    VectorSearchEndpointExecutor,\n    VectorSearchIndexExecutor,\n    ServicePrincipalExecutor,\n    get_privileged_client,\n)\nfrom brickkit.models.base import set_current_environment\nfrom brickkit.models.enums import Environment\n\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# Set BrickKit environment\nENV_MAP = {\"dev\": Environment.DEV, \"acc\": Environment.ACC, \"prd\": Environment.PRD}\nset_current_environment(ENV_MAP[ENVIRONMENT])\n\nprint(f\"BrickKit environment set to: {ENVIRONMENT}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD GOVERNANCE CONVENTION ===\n",
    "# The convention defines naming patterns, required tags, and ownership rules\n",
    "\n",
    "if IS_DATABRICKS:\n",
    "    CONVENTION_PATH = os.path.join(workspace_notebook_dir, \"conventions\", \"financial_services.yml\")\n",
    "else:\n",
    "    CONVENTION_PATH = str(notebook_dir / \"conventions\" / \"financial_services.yml\")\n",
    "\n",
    "convention = load_convention(CONVENTION_PATH)\n",
    "\n",
    "print(f\"Loaded convention: {convention.name} (v{convention.version})\")\n",
    "print(f\"Rules: {len(convention.schema.rules)}\")\n",
    "print(f\"Default tags: {len(convention.schema.tags)}\")\n",
    "\n",
    "# Show what the convention enforces\n",
    "for rule in convention.schema.rules:\n",
    "    mode = \"ENFORCED\" if rule.mode.value == \"enforced\" else \"ADVISORY\"\n",
    "    print(f\"  [{mode}] {rule.rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap: Admin Service Principal\n",
    "\n",
    "Notebook tokens have limited permissions (e.g., cannot update catalog isolation mode).\n",
    "We create an \"admin\" service principal with OAuth credentials stored in Databricks Secrets.\n",
    "\n",
    "**Important: Grants to Service Principals require `application_id`**\n",
    "\n",
    "Databricks grants API requires the `application_id` (UUID) for service principals, not the display name.\n",
    "BrickKit handles this automatically:\n",
    "1. After creating an SPN, the `application_id` is stored on the `ManagedServicePrincipal`\n",
    "2. Use `spn.to_principal()` to get a `Principal` with the `application_id` set\n",
    "3. The `Principal.resolved_name` returns the `application_id` for grants\n",
    "\n",
    "**Bootstrap flow:**\n",
    "1. Create admin SPN using your user token\n",
    "2. Store SPN credentials in Databricks Secrets\n",
    "3. Get Principal with application_id via `admin_spn.to_principal()`\n",
    "4. Grant the admin SPN `MANAGE` + `USE_CATALOG` on the catalog\n",
    "5. Create privileged client from stored credentials\n",
    "6. Use privileged client for catalog operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === BOOTSTRAP: ADMIN SERVICE PRINCIPAL ===\n# Creates an admin SPN with OAuth credentials for privileged operations.\n# IMPORTANT: For grants to service principals, Databricks requires the application_id (UUID),\n# not the display name. After creating the SPN, we use to_principal() to get a Principal\n# with the application_id set.\n\nADMIN_SPN_NAME = \"spn_brickkit_admin\"\nSECRET_SCOPE = \"brickkit\"\n\nadmin_spn = ManagedServicePrincipal(name=ADMIN_SPN_NAME)\nadmin_spn.add_entitlement(\"workspace-access\")\nadmin_spn.add_entitlement(\"databricks-sql-access\")\n\n# Use profile for local, default for Databricks\nif IS_DATABRICKS:\n    bootstrap_client = WorkspaceClient()\nelse:\n    bootstrap_client = WorkspaceClient(profile=DATABRICKS_PROFILE)\n\nspn_executor = ServicePrincipalExecutor(bootstrap_client, dry_run=DRY_RUN)\n\n# Check if SPN + credentials already exist\nspn_exists = spn_executor.exists(admin_spn)\n\n# Get secrets - different approach for local vs Databricks\ndef get_secret(scope: str, key: str) -> str | None:\n    \"\"\"Get secret from Databricks or environment variable for local.\"\"\"\n    if IS_DATABRICKS:\n        try:\n            return dbutils.secrets.get(scope=scope, key=key)\n        except Exception:\n            return None\n    else:\n        # Local: try environment variables first, then fall back to SDK\n        env_key = f\"{scope.upper()}_{key.upper().replace('-', '_')}\"\n        if env_key in os.environ:\n            return os.environ[env_key]\n        # Try using the SDK to list secrets (won't get values, but can check existence)\n        return None\n\nADMIN_SPN_APP_ID = get_secret(SECRET_SCOPE, \"admin-spn-client-id\")\ncredentials_exist = spn_exists and ADMIN_SPN_APP_ID is not None\n\n# Create SPN and store credentials if needed\nif not credentials_exist and not DRY_RUN:\n    print(f\"Creating admin SPN: {admin_spn.resolved_name}\")\n    result, credentials = spn_executor.create_with_secret(admin_spn)\n    print(f\"  {result.operation.value}: {result.message}\")\n    \n    if credentials:\n        if IS_DATABRICKS:\n            spn_executor.store_credentials(credentials, scope=SECRET_SCOPE)\n            print(f\"  Stored credentials in Databricks secrets\")\n        else:\n            print(f\"  WARNING: Cannot store credentials locally. Set these env vars:\")\n            print(f\"    export BRICKKIT_ADMIN_SPN_CLIENT_ID={credentials.application_id}\")\n            print(f\"    export BRICKKIT_ADMIN_SPN_CLIENT_SECRET=<secret>\")\n        ADMIN_SPN_APP_ID = credentials.application_id\n        print(f\"  Application ID: {ADMIN_SPN_APP_ID}\")\nelse:\n    # SPN exists - ensure we have the application_id on the model\n    if ADMIN_SPN_APP_ID:\n        admin_spn.application_id = ADMIN_SPN_APP_ID\n    else:\n        # Look it up from Databricks\n        result = spn_executor.create(admin_spn)  # This will set application_id on admin_spn\n        ADMIN_SPN_APP_ID = admin_spn.application_id\n    print(f\"Admin SPN ready: {admin_spn.resolved_name} (app_id: {ADMIN_SPN_APP_ID})\")\n\n# Create Principal with application_id for use in grants\n# Grants to service principals require the application_id (UUID), not the display name\nadmin_spn_principal = admin_spn.to_principal()\nprint(f\"  Admin principal for grants: {admin_spn_principal.resolved_name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Team, Workspace & Principals\n",
    "\n",
    "BrickKit uses `Team` to bring together:\n",
    "- **Workspace** - The Databricks workspace(s) per environment\n",
    "- **Service Principals** - For ownership and access (account-level compatible)\n",
    "- **Catalog bindings** - Automatically configured via `team.add_catalog()`\n",
    "\n",
    "**Why SPNs only?** Unity Catalog requires account-level principals for both ownership AND grants. Service Principals work at both workspace and account level. Workspace groups (created via SCIM) are workspace-local and cannot be used with Unity Catalog. In production with IdP integration, account-level groups synced via SCIM would also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === DEFINE WORKSPACE ===\nif IS_DATABRICKS:\n    WORKSPACE_HOSTNAME = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\nelse:\n    # Local: get hostname from the SDK client config\n    WORKSPACE_HOSTNAME = bootstrap_client.config.host.replace(\"https://\", \"\")\n\nregistry = WorkspaceRegistry()\ndev_workspace = registry.get_or_create(\n    workspace_id=WORKSPACE_ID,\n    name=WORKSPACE_NAME,\n    hostname=WORKSPACE_HOSTNAME,\n    environment=Environment.DEV,\n)\n\nprint(f\"Workspace: {dev_workspace.name} (ID: {dev_workspace.workspace_id})\")\nprint(f\"  Hostname: {WORKSPACE_HOSTNAME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEFINE PRINCIPALS ===\n",
    "# Service principal for resource ownership.\n",
    "# Environment-aware: names automatically get _dev/_acc/_prd suffixes.\n",
    "#\n",
    "# NOTE: Unity Catalog requires account-level principals for ownership AND grants.\n",
    "# Service Principals work at both workspace and account level, making them\n",
    "# the most reliable choice for automated deployments.\n",
    "\n",
    "# Service Principal for catalog AND schema ownership (per convention)\n",
    "trading_platform_spn = ManagedServicePrincipal(name=\"spn_trading_platform\")\n",
    "trading_platform_spn.add_entitlement(\"workspace-access\")\n",
    "trading_platform_spn.add_entitlement(\"databricks-sql-access\")\n",
    "\n",
    "print(f\"Service Principal (owner): {trading_platform_spn.resolved_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY TEAM PRINCIPALS ===\n",
    "# Create the service principal in Databricks before using it as owner.\n",
    "# The executor is idempotent - returns SKIPPED if principal already exists.\n",
    "\n",
    "# Deploy trading platform SPN (will be catalog AND schema owner)\n",
    "print(f\"Deploying service principal: {trading_platform_spn.resolved_name}\")\n",
    "result = spn_executor.create(trading_platform_spn)\n",
    "print(f\"  {result.operation.value}: {result.message}\")\n",
    "\n",
    "# NOTE: We skip creating the workspace group since Unity Catalog\n",
    "# requires account-level principals for both ownership AND grants.\n",
    "# In production, use account-level groups synced via IdP SCIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEFINE TEAM ===\n",
    "# Team brings together workspace, principals, and manages catalog bindings.\n",
    "\n",
    "# After SPN deployment (cell 11), the application_id is set on trading_platform_spn.\n",
    "# Use to_principal() to get a Principal with the application_id for ownership/grants.\n",
    "# Both catalog AND schema are owned by the same SPN (per convention: owner_must_be_sp)\n",
    "\n",
    "catalog_owner = trading_platform_spn.to_principal()\n",
    "schema_owner = trading_platform_spn.to_principal()  # Same SPN owns schemas too\n",
    "\n",
    "# Create team and add workspace + principals\n",
    "quant_team = Team(name=\"quant_trading\")\n",
    "quant_team.add_workspace(dev_workspace)\n",
    "quant_team.add_principal(catalog_owner)\n",
    "\n",
    "print(f\"Team: {quant_team.name}\")\n",
    "print(f\"  Workspaces: {list(quant_team.workspaces.keys())}\")\n",
    "print(f\"  Catalog/Schema owner: {catalog_owner.display_name} (app_id: {catalog_owner.application_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Sample Data\n",
    "\n",
    "We'll use a small inline dataset of World Bank indicators. This lets you run the full demo quickly without external API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAMPLE DATA ===\n",
    "# 20 World Bank indicators with embedding text for vector search\n",
    "\n",
    "SAMPLE_INDICATORS = [\n",
    "    (\"SP.POP.TOTL\", \"Population, total\", \"Total population counts all residents regardless of legal status or citizenship.\", \"Demographics\"),\n",
    "    (\"NY.GDP.MKTP.CD\", \"GDP (current US$)\", \"GDP at purchaser's prices is the sum of gross value added by all resident producers.\", \"Economy\"),\n",
    "    (\"NY.GDP.PCAP.CD\", \"GDP per capita (current US$)\", \"GDP per capita is gross domestic product divided by midyear population.\", \"Economy\"),\n",
    "    (\"SI.POV.DDAY\", \"Poverty headcount ratio at $2.15 a day\", \"Poverty headcount ratio at $2.15 a day is the percentage of the population living on less than $2.15 a day.\", \"Poverty\"),\n",
    "    (\"SI.POV.GINI\", \"Gini index\", \"Gini index measures the extent to which the distribution of income among individuals deviates from a perfectly equal distribution.\", \"Inequality\"),\n",
    "    (\"SL.UEM.TOTL.ZS\", \"Unemployment, total (% of labor force)\", \"Unemployment refers to the share of the labor force that is without work but available and seeking employment.\", \"Labor\"),\n",
    "    (\"FP.CPI.TOTL.ZG\", \"Inflation, consumer prices (annual %)\", \"Inflation as measured by the consumer price index reflects the annual percentage change in the cost of goods and services.\", \"Economy\"),\n",
    "    (\"SP.DYN.LE00.IN\", \"Life expectancy at birth, total (years)\", \"Life expectancy at birth indicates the number of years a newborn infant would live if patterns of mortality at birth were to stay the same.\", \"Health\"),\n",
    "    (\"SH.DYN.MORT\", \"Mortality rate, under-5 (per 1,000 live births)\", \"Under-five mortality rate is the probability per 1,000 that a newborn baby will die before reaching age five.\", \"Health\"),\n",
    "    (\"SE.ADT.LITR.ZS\", \"Literacy rate, adult total (% of people ages 15 and above)\", \"Adult literacy rate is the percentage of people ages 15 and above who can read and write a short simple statement.\", \"Education\"),\n",
    "    (\"SE.PRM.ENRR\", \"School enrollment, primary (% gross)\", \"Gross enrollment ratio is the ratio of total enrollment to the population of the age group that officially corresponds to the level of education.\", \"Education\"),\n",
    "    (\"EG.USE.ELEC.KH.PC\", \"Electric power consumption (kWh per capita)\", \"Electric power consumption measures the production of power plants and combined heat and power plants less transmission losses.\", \"Energy\"),\n",
    "    (\"EN.ATM.CO2E.PC\", \"CO2 emissions (metric tons per capita)\", \"Carbon dioxide emissions are those stemming from the burning of fossil fuels and the manufacture of cement.\", \"Environment\"),\n",
    "    (\"AG.LND.FRST.ZS\", \"Forest area (% of land area)\", \"Forest area is land under natural or planted stands of trees of at least 5 meters in situ.\", \"Environment\"),\n",
    "    (\"SH.XPD.CHEX.PC.CD\", \"Current health expenditure per capita (current US$)\", \"Current expenditures on health per capita in current US dollars.\", \"Health\"),\n",
    "    (\"IT.NET.USER.ZS\", \"Individuals using the Internet (% of population)\", \"Internet users are individuals who have used the Internet in the last 3 months.\", \"Technology\"),\n",
    "    (\"BX.KLT.DINV.CD.WD\", \"Foreign direct investment, net inflows (BoP, current US$)\", \"Foreign direct investment are the net inflows of investment to acquire a lasting management interest.\", \"Economy\"),\n",
    "    (\"GC.DOD.TOTL.GD.ZS\", \"Central government debt, total (% of GDP)\", \"Debt is the entire stock of direct government fixed-term contractual obligations to others outstanding.\", \"Economy\"),\n",
    "    (\"NE.EXP.GNFS.ZS\", \"Exports of goods and services (% of GDP)\", \"Exports of goods and services represent the value of all goods and other market services provided to the rest of the world.\", \"Trade\"),\n",
    "    (\"NE.IMP.GNFS.ZS\", \"Imports of goods and services (% of GDP)\", \"Imports of goods and services represent the value of all goods and other market services received from the rest of the world.\", \"Trade\"),\n",
    "]\n",
    "\n",
    "INDICATORS_SCHEMA = StructType([\n",
    "    StructField(\"indicator_id\", StringType(), False),\n",
    "    StructField(\"indicator_name\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"topic\", StringType(), True),\n",
    "    StructField(\"embedding_text\", StringType(), True),\n",
    "])\n",
    "\n",
    "def create_sample_dataframe(spark_session: SparkSession):\n",
    "    \"\"\"Create DataFrame from sample indicators with embedding text.\"\"\"\n",
    "    rows = [(ind_id, name, desc, topic, f\"{name}. {desc}\") for ind_id, name, desc, topic in SAMPLE_INDICATORS]\n",
    "    return spark_session.createDataFrame(rows, INDICATORS_SCHEMA)\n",
    "\n",
    "print(f\"Sample data defined: {len(SAMPLE_INDICATORS)} indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Fetch Real Data from World Bank API\n",
    "\n",
    "Uncomment and run the cell below to fetch real indicator metadata. This takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTIONAL: FETCH FROM WORLD BANK API ===\n",
    "# Uncomment this cell to fetch real data (takes several minutes)\n",
    "\n",
    "# %pip install wbgapi requests tqdm --quiet\n",
    "\n",
    "# import wbgapi as wb\n",
    "# import requests\n",
    "# from requests.exceptions import RequestException, Timeout\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def fetch_worldbank_indicators(spark: SparkSession, limit: int = 100):\n",
    "#     \"\"\"Fetch indicator metadata from World Bank API.\"\"\"\n",
    "#     series_list = wb.series.info()\n",
    "#     series_ids = [s.get(\"id\") for s in series_list.items][:limit]\n",
    "#\n",
    "#     rows = []\n",
    "#     for series_id in tqdm(series_ids, desc=\"Fetching\"):\n",
    "#         try:\n",
    "#             url = f\"https://api.worldbank.org/v2/indicator/{series_id}?format=json\"\n",
    "#             resp = requests.get(url, timeout=30)\n",
    "#             resp.raise_for_status()\n",
    "#             data = resp.json()\n",
    "#             if len(data) >= 2 and data[1]:\n",
    "#                 meta = data[1][0]\n",
    "#                 name = meta.get(\"name\", \"\") or \"\"\n",
    "#                 desc = meta.get(\"sourceNote\", \"\") or \"\"\n",
    "#                 topics = meta.get(\"topics\", []) or []\n",
    "#                 topic = topics[0].get(\"value\", \"\") if topics else \"\"\n",
    "#                 embedding_text = f\"{name}. {desc}\".strip()\n",
    "#                 rows.append((series_id, name, desc, topic, embedding_text))\n",
    "#         except (RequestException, Timeout, ValueError) as e:\n",
    "#             print(f\"Skipping {series_id}: {e}\")\n",
    "#\n",
    "#     return spark.createDataFrame(rows, INDICATORS_SCHEMA)\n",
    "\n",
    "# # Fetch real data (uncomment to use)\n",
    "# sample_df = fetch_worldbank_indicators(spark, limit=500)\n",
    "# print(f\"Fetched {sample_df.count()} indicators from World Bank API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Governed Resources\n",
    "\n",
    "Now we define our resources using BrickKit models. The convention automatically applies:\n",
    "- Environment-specific naming (e.g., `quant_risk_dev`)\n",
    "- Required governance tags\n",
    "- Ownership rules validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ENVIRONMENT SETUP ===\n",
    "# catalog_owner and schema_owner are already defined in the Team cell above\n",
    "\n",
    "environment = ENV_MAP[ENVIRONMENT]\n",
    "\n",
    "print(f\"Catalog owner: {catalog_owner.resolved_name} ({catalog_owner.principal_type.value})\")\n",
    "print(f\"Schema owner: {schema_owner.resolved_name} ({schema_owner.principal_type.value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CATALOG ===\n",
    "# NOTE: In Free Edition workspaces with \"Default Storage\", catalogs cannot be created via SDK.\n",
    "# Create the catalog manually via UI first (with \"Use default storage\" checked), then reference it here.\n",
    "\n",
    "catalog_name = convention.generate_name(SecurableType.CATALOG, environment)\n",
    "\n",
    "catalog = Catalog(\n",
    "    name=catalog_name,\n",
    "    owner=catalog_owner,\n",
    "    comment=\"Risk Analytics catalog for quantitative trading\",\n",
    "    isolation_mode=IsolationMode.ISOLATED,  # Default Storage catalogs are workspace-isolated\n",
    ")\n",
    "\n",
    "# Use Team to automatically configure workspace bindings\n",
    "# This sets catalog.workspace_ids based on the team's workspace for this environment\n",
    "quant_team.add_catalog(catalog)\n",
    "\n",
    "# Apply convention (adds tags, validates rules)\n",
    "convention.apply_to(catalog, environment)\n",
    "errors = convention.get_validation_errors(catalog)\n",
    "if errors:\n",
    "    raise ValueError(f\"Catalog validation failed: {errors}\")\n",
    "\n",
    "print(f\"Catalog: {catalog.name}\")\n",
    "print(f\"  Isolation Mode: {catalog.isolation_mode.value}\")\n",
    "print(f\"  Workspace IDs: {catalog.workspace_ids} (auto-configured by Team)\")\n",
    "print(f\"  Tags: {len(catalog.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SCHEMA ===\n",
    "schema = Schema(\n",
    "    name=SCHEMA_NAME,\n",
    "    catalog_name=catalog.name,\n",
    "    owner=schema_owner,\n",
    "    comment=\"World Bank indicator metadata for vector search\",\n",
    ")\n",
    "\n",
    "convention.apply_to(schema, environment)\n",
    "errors = convention.get_validation_errors(schema)\n",
    "if errors:\n",
    "    raise ValueError(f\"Schema validation failed: {errors}\")\n",
    "\n",
    "print(f\"Schema: {schema.fqdn}\")\n",
    "print(f\"  Tags: {len(schema.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TABLE ===\n",
    "# Define the table structure with BrickKit (not just raw PySpark write)\n",
    "\n",
    "table = Table(\n",
    "    name=TABLE_NAME,\n",
    "    catalog_name=catalog.name,\n",
    "    schema_name=schema.name,\n",
    "    owner=schema_owner,\n",
    "    comment=\"World Bank indicator metadata with embeddings for semantic search\",\n",
    "    columns=[\n",
    "        ColumnInfo(name=\"indicator_id\", type=\"STRING\", nullable=False, comment=\"World Bank indicator code\"),\n",
    "        ColumnInfo(name=\"indicator_name\", type=\"STRING\", nullable=True, comment=\"Human-readable indicator name\"),\n",
    "        ColumnInfo(name=\"description\", type=\"STRING\", nullable=True, comment=\"Full description of the indicator\"),\n",
    "        ColumnInfo(name=\"topic\", type=\"STRING\", nullable=True, comment=\"Category/topic of the indicator\"),\n",
    "        ColumnInfo(name=\"embedding_text\", type=\"STRING\", nullable=True, comment=\"Text used for embedding generation\"),\n",
    "    ],\n",
    "    tags=[\n",
    "        Tag(key=\"data_source\", value=\"worldbank_api\"),\n",
    "        Tag(key=\"refresh_frequency\", value=\"weekly\"),\n",
    "        Tag(key=\"contains_pii\", value=\"false\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "convention.apply_to(table, environment)\n",
    "errors = convention.get_validation_errors(table)\n",
    "if errors:\n",
    "    raise ValueError(f\"Table validation failed: {errors}\")\n",
    "\n",
    "print(f\"Table: {table.fqdn}\")\n",
    "print(f\"  Columns: {len(table.columns)}\")\n",
    "print(f\"  Tags: {len(table.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VECTOR SEARCH ENDPOINT ===\n",
    "vs_endpoint = VectorSearchEndpoint(\n",
    "    name=ENDPOINT_NAME,\n",
    "    comment=\"Semantic search endpoint for risk analytics indicators\",\n",
    "    # NOTE: Endpoints don't support custom tags\n",
    ")\n",
    "\n",
    "convention.apply_to(vs_endpoint, environment)\n",
    "errors = convention.get_validation_errors(vs_endpoint)\n",
    "if errors:\n",
    "    raise ValueError(f\"Endpoint validation failed: {errors}\")\n",
    "\n",
    "print(f\"Endpoint: {vs_endpoint.resolved_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VECTOR SEARCH INDEX ===\n",
    "# Use table.fqdn to reference the governed table\n",
    "\n",
    "vs_index = VectorSearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    source_table=table.fqdn,  # Reference the governed Table model\n",
    "    primary_key=\"indicator_id\",\n",
    "    embedding_column=\"embedding_text\",\n",
    "    embedding_model=\"databricks-bge-large-en\",\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    # NOTE: Skipping tags for now - SDK support TBD\n",
    ")\n",
    "\n",
    "convention.apply_to(vs_index, environment)\n",
    "errors = convention.get_validation_errors(vs_index)\n",
    "if errors:\n",
    "    raise ValueError(f\"Index validation failed: {errors}\")\n",
    "\n",
    "print(f\"Index: {vs_index.resolved_name}\")\n",
    "print(f\"  Source: {vs_index.source_table}\")\n",
    "print(f\"  Endpoint: {vs_index.resolved_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deploy with BrickKit Executors\n",
    "\n",
    "BrickKit executors handle:\n",
    "- Idempotent create (skip if exists)\n",
    "- Wait for provisioning\n",
    "- Tag application\n",
    "- **Permission grants** - Ensure teams have access after ownership change\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === INITIALIZE CLIENTS AND EXECUTORS ===\n\nif IS_DATABRICKS:\n    ws_client = WorkspaceClient()\nelse:\n    ws_client = WorkspaceClient(profile=DATABRICKS_PROFILE)\n    \nvs_client = VectorSearchClient()\n\n# Create privileged client from stored SPN credentials\n# On Databricks: uses dbutils to get secrets\n# Locally: uses the same profile (user auth) - may have limited permissions\nif IS_DATABRICKS:\n    privileged_client = get_privileged_client(\n        host=WORKSPACE_HOSTNAME,\n        scope=SECRET_SCOPE,\n        dbutils=dbutils,\n    )\nelse:\n    # Local: use the same client (your user has permissions)\n    # For full SPN-based auth locally, set BRICKKIT_ADMIN_SPN_CLIENT_ID and _SECRET env vars\n    client_id = os.environ.get(\"BRICKKIT_ADMIN_SPN_CLIENT_ID\")\n    client_secret = os.environ.get(\"BRICKKIT_ADMIN_SPN_CLIENT_SECRET\")\n    if client_id and client_secret:\n        privileged_client = WorkspaceClient(\n            host=f\"https://{WORKSPACE_HOSTNAME}\",\n            client_id=client_id,\n            client_secret=client_secret,\n        )\n        print(f\"Using SPN-based privileged client\")\n    else:\n        privileged_client = ws_client\n        print(f\"Using user-based client (no SPN credentials in env)\")\n\n# Grant OWNER_ADMIN (ALL_PRIVILEGES + MANAGE) to the admin SPN on the catalog\nprint(f\"Granting admin SPN access to catalog {catalog.resolved_name}...\")\ncatalog.grant(admin_spn_principal, AccessPolicy.OWNER_ADMIN())\n\nbootstrap_grant_executor = GrantExecutor(ws_client, dry_run=False)\nfor result in bootstrap_grant_executor.apply_privileges(catalog.privileges):\n    print(f\"  {result.operation.value}: {result.message}\")\n\n# Initialize executors\ncatalog_executor = CatalogExecutor(privileged_client, dry_run=DRY_RUN)\nschema_executor = SchemaExecutor(privileged_client, dry_run=DRY_RUN)\ngrant_executor = GrantExecutor(ws_client, dry_run=DRY_RUN)\nendpoint_executor = VectorSearchEndpointExecutor(ws_client, dry_run=DRY_RUN)\nindex_executor = VectorSearchIndexExecutor(ws_client, dry_run=DRY_RUN)\n\nprint(f\"Executors initialized (dry_run={DRY_RUN})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY CATALOG ===\n",
    "# In Free Edition with \"Default Storage\", create the catalog manually via UI first.\n",
    "# The executor handles existing catalogs gracefully (returns NO_OP if already exists).\n",
    "result = catalog_executor.create(catalog)\n",
    "print(f\"Catalog: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY SCHEMA ===\n",
    "result = schema_executor.create(schema)\n",
    "print(f\"Schema: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VERIFY GRANTS ===\n",
    "# The admin SPN already has OWNER_ADMIN access (granted in cell 25).\n",
    "# That's sufficient for this demo.\n",
    "#\n",
    "# NOTE: Unity Catalog requires account-level principals for grants.\n",
    "# Workspace groups (created via SCIM) cannot receive UC grants.\n",
    "# In production, you would:\n",
    "# 1. Use account-level groups (synced from IdP via SCIM at account level)\n",
    "# 2. Or grant to individual users\n",
    "# 3. Or use additional service principals\n",
    "\n",
    "# Show what privileges are configured on the catalog\n",
    "print(f\"Privileges on catalog {catalog.resolved_name}:\")\n",
    "for priv in catalog.privileges:\n",
    "    print(f\"  - {priv.privilege.value} to {priv.principal}\")\n",
    "\n",
    "print(f\"\\n✓ Admin SPN has full access to manage resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WRITE DATA TO TABLE ===\n",
    "sample_df = create_sample_dataframe(spark)\n",
    "\n",
    "sample_df.write.format(\"delta\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(table.fqdn)\n",
    "\n",
    "print(f\"✓ Table {table.fqdn}: {spark.table(table.fqdn).count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY VECTOR SEARCH ENDPOINT ===\n",
    "result = endpoint_executor.create(vs_endpoint)\n",
    "print(f\"Endpoint: {result.operation.value} - {result.message}\")\n",
    "\n",
    "# Wait for endpoint to be online (uses executor's built-in wait logic)\n",
    "if not DRY_RUN and result.operation.value == \"CREATE\":\n",
    "    print(\"Waiting for endpoint to be online...\")\n",
    "    if endpoint_executor.wait_for_endpoint(vs_endpoint):\n",
    "        print(f\"Endpoint {vs_endpoint.resolved_name} is ONLINE\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Endpoint {vs_endpoint.resolved_name} failed to provision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY VECTOR SEARCH INDEX ===\n",
    "result = index_executor.create(vs_index)\n",
    "print(f\"Index: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test Vector Search\n",
    "\n",
    "The index syncs asynchronously. Once ready, we can run similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECK INDEX STATUS ===\n",
    "FULL_INDEX_NAME = f\"{catalog.name}.{schema.name}.{vs_index.resolved_name}\"\n",
    "\n",
    "index = vs_client.get_index(\n",
    "    endpoint_name=vs_endpoint.resolved_name,\n",
    "    index_name=FULL_INDEX_NAME,\n",
    ")\n",
    "status = index.describe().get(\"status\", {})\n",
    "print(f\"Index status: ready={status.get('ready', 'UNKNOWN')}, message={status.get('message', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN SIMILARITY SEARCH ===\n",
    "TEST_QUERY = \"poverty and inequality measures\"\n",
    "\n",
    "results = index.similarity_search(\n",
    "    query_text=TEST_QUERY,\n",
    "    columns=[\"indicator_id\", \"indicator_name\", \"description\", \"topic\"],\n",
    "    num_results=5,\n",
    ")\n",
    "\n",
    "print(f\"Search: '{TEST_QUERY}'\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in enumerate(results.get(\"result\", {}).get(\"data_array\", []), 1):\n",
    "    print(f\"{i}. [{row[3]}] {row[1]}\")\n",
    "    print(f\"   {row[2][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. What BrickKit Added (Governance Value)\n",
    "\n",
    "Let's see what governance BrickKit applied automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GOVERNANCE SUMMARY ===\n",
    "\n",
    "def print_tags(resource):\n",
    "    if resource.tags:\n",
    "        print(f\"  Tags: {', '.join(f'{t.key}={t.value}' for t in resource.tags)}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEAM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Name: {quant_team.name}\")\n",
    "print(f\"  Workspaces: {[(e.value, w.name) for e, w in quant_team.workspaces.items()]}\")\n",
    "print(f\"  Principals: {[p.resolved_name for p in quant_team.principals]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CATALOG\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Name: {catalog.resolved_name}\")\n",
    "print(f\"  Owner: {catalog.owner.resolved_name}\")\n",
    "print(f\"  Isolation: {catalog.isolation_mode.value}\")\n",
    "print(f\"  Workspaces: {catalog.workspace_ids}\")\n",
    "print_tags(catalog)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Name: {schema.fqdn}\")\n",
    "print(f\"  Owner: {schema.owner.resolved_name}\")\n",
    "print_tags(schema)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TABLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Name: {table.fqdn}\")\n",
    "print(f\"  Columns: {len(table.columns)}\")\n",
    "print_tags(table)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VECTOR SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Endpoint: {vs_endpoint.resolved_name}\")\n",
    "print(f\"  Index: {vs_index.resolved_name}\")\n",
    "print(f\"  Source: {vs_index.source_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONVENTION RULES APPLIED ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONVENTION RULES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Convention: {convention.name} (v{convention.version})\")\n",
    "print()\n",
    "\n",
    "for rule in convention.schema.rules:\n",
    "    mode = \"ENFORCED\" if rule.mode.value == \"enforced\" else \"ADVISORY\"\n",
    "    print(f\"[{mode}] {rule.rule}\")\n",
    "\n",
    "print()\n",
    "print(\"What this means:\")\n",
    "print(\"- All securables MUST be owned by service principals (not users or groups)\")\n",
    "print(\"- This ensures compatibility with Unity Catalog's account-level principal requirements\")\n",
    "print(\"- Resources SHOULD have cost_center and team tags\")\n",
    "print(\"- BrickKit validated all these rules before deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WHAT YOU DIDN'T HAVE TO DO ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHAT BRICKKIT DID FOR YOU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "benefits = [\n",
    "    (\"Team definition\", f\"Team '{quant_team.name}' brings together workspace + principals\"),\n",
    "    (\"Principal definitions\", f\"Defined SPN with application_id for ownership\"),\n",
    "    (\"Workspace binding\", f\"team.add_catalog() auto-configured workspace IDs: {catalog.workspace_ids}\"),\n",
    "    (\"Environment suffixes\", f\"All names automatically suffixed with '_{ENVIRONMENT}'\"),\n",
    "    (\"Governance tags\", f\"{len(catalog.tags)} tags auto-applied from convention\"),\n",
    "    (\"Ownership validation\", \"Verified all securables have SPN owners (Unity Catalog compatible)\"),\n",
    "    (\"Permission grants\", f\"Admin SPN has OWNER_ADMIN for full access\"),\n",
    "    (\"Request for Access\", \"RFA configured with inheritance (table inherits from schema)\"),\n",
    "    (\"Idempotent deployment\", \"Executors skip if resource exists, sync tags if needed\"),\n",
    "    (\"Wait logic\", \"Built-in endpoint provisioning wait with timeout/retry\"),\n",
    "    (\"Consistent patterns\", \"Same governance across Catalog, Schema, Endpoint, Index\"),\n",
    "]\n",
    "\n",
    "for benefit, detail in benefits:\n",
    "    print(f\"\\n{benefit}:\")\n",
    "    print(f\"  {detail}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Without BrickKit, you would manually:\")\n",
    "print(\"  - Define SPNs with raw SDK calls and track application_ids\")\n",
    "print(\"  - Track which workspace IDs to bind to each catalog\")\n",
    "print(\"  - Add environment suffixes to every resource name\")\n",
    "print(\"  - Remember which tags to apply (and apply them consistently)\")\n",
    "print(\"  - Validate ownership rules before deployment\")\n",
    "print(\"  - Grant permissions after changing ownership\")\n",
    "print(\"  - Configure RFA on each securable individually\")\n",
    "print(\"  - Write wait/retry logic for endpoint provisioning\")\n",
    "print(\"  - Handle idempotency (check exists, update tags, etc.)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This demo showed:\n",
    "\n",
    "1. **Convention Loading** - Governance rules from YAML\n",
    "2. **Team Definition** - `Team` with `Workspace` and `Principal` members\n",
    "3. **Principal Definition** - `ManagedServicePrincipal`, `ManagedGroup` with members\n",
    "4. **Catalog Binding** - `team.add_catalog()` auto-configures workspace IDs\n",
    "5. **Governed Models** - `Catalog`, `Schema`, `Table`, `VectorSearchEndpoint`, `VectorSearchIndex`\n",
    "6. **Executors** - Idempotent deployment with built-in wait logic\n",
    "7. **Permission Grants** - Ensure team access after ownership changes\n",
    "8. **Automatic Governance** - Tags, naming, ownership validation\n",
    "\n",
    "### BrickKit vs DAB Recap\n",
    "\n",
    "- **DAB** handles: notebook sync, job definitions, workflow orchestration\n",
    "- **BrickKit** handles: teams, principals, catalog/schema/table creation, workspace bindings, VS endpoint/index, grants, tags, validation\n",
    "- **Together**: DAB runs this notebook as a job, BrickKit deploys the governed resources\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Modify `conventions/financial_services.yml` to change governance rules\n",
    "- Set `dry_run=false` to deploy for real\n",
    "- Try different environments (`dev`, `acc`, `prd`) to see naming changes\n",
    "- Add more workspaces to the team for multi-environment deployments\n",
    "- Add your own data source instead of sample indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}