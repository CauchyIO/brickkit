{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search with BrickKit\n",
    "\n",
    "This notebook demonstrates **end-to-end usage of BrickKit** for deploying a governed Vector Search solution.\n",
    "\n",
    "## What BrickKit Does\n",
    "\n",
    "BrickKit automates governance for Databricks resources:\n",
    "- **Team management** - Define teams with workspaces and principals, auto-configure catalog bindings\n",
    "- **Principal management** - Define groups and service principals with environment-aware naming\n",
    "- **Naming conventions** - Environment-aware names (dev/acc/prd suffixes)\n",
    "- **Tagging** - Automatic cost center, team, compliance tags\n",
    "- **Ownership rules** - Enforce service principals for catalogs, groups for schemas\n",
    "- **Permission grants** - Ensure teams retain access after ownership changes\n",
    "- **Request for Access (RFA)** - Configure access request destinations with inheritance\n",
    "- **Validation** - Catch governance violations before deployment\n",
    "\n",
    "## BrickKit vs DAB (Databricks Asset Bundles)\n",
    "\n",
    "| Resource | DAB | BrickKit | Notes |\n",
    "|----------|-----|----------|-------|\n",
    "| **Teams** | Not supported | Defines & manages | BrickKit organizes workspace + principals |\n",
    "| **Groups/SPNs** | Not supported | Defines & manages | BrickKit defines principals declaratively |\n",
    "| **Catalog** | References only | Creates & governs | DAB passes variables, BrickKit deploys |\n",
    "| **Schema** | References only | Creates & governs | Same |\n",
    "| **Table** | References only | Creates & governs | BrickKit defines structure + tags |\n",
    "| **VS Endpoint** | Not supported | Creates & governs | DAB can't create these |\n",
    "| **VS Index** | Not supported | Creates & governs | DAB can't create these |\n",
    "| **Jobs/Workflows** | Defines | N/A | DAB's strength |\n",
    "| **Notebook sync** | Deploys | N/A | DAB syncs to workspace |\n",
    "\n",
    "**Key insight:** DAB deploys *code assets* (notebooks, jobs). BrickKit deploys *data assets* (catalogs, tables, VS) and *principals* (teams, groups, SPNs).\n",
    "\n",
    "## What This Demo Shows\n",
    "\n",
    "1. Load a governance convention from YAML\n",
    "2. **Define Team** with workspace and principals\n",
    "3. Define governed resources (Catalog, Schema, Table, VS Endpoint, VS Index)\n",
    "4. **Auto-configure workspace bindings** via `team.add_catalog()`\n",
    "5. Deploy using BrickKit executors\n",
    "6. **Grant permissions** to team groups after ownership change\n",
    "7. Test vector search\n",
    "8. See what governance BrickKit applied automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade databricks-vectorsearch databricks-sdk pydantic pyyaml --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Edit these widgets or override via DAB job parameters\n",
    "\n",
    "dbutils.widgets.text(\"catalog\", \"quant_risk\", \"Catalog Name (base)\")\n",
    "dbutils.widgets.text(\"schema\", \"indicators\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"endpoint_name\", \"worldbank_vector_search\", \"VS Endpoint Name\")\n",
    "dbutils.widgets.text(\"managed_location\", \"\", \"Managed Location (for Default Storage workspaces)\")\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"acc\", \"prd\"], \"Environment\")\n",
    "dbutils.widgets.dropdown(\"dry_run\", \"false\", [\"true\", \"false\"], \"Dry Run\")\n",
    "\n",
    "# Read widget values\n",
    "CATALOG_BASE = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema\")\n",
    "ENDPOINT_NAME = dbutils.widgets.get(\"endpoint_name\")\n",
    "MANAGED_LOCATION = dbutils.widgets.get(\"managed_location\") or None  # Convert empty string to None\n",
    "ENVIRONMENT = dbutils.widgets.get(\"environment\")\n",
    "DRY_RUN = dbutils.widgets.get(\"dry_run\").lower() == \"true\"\n",
    "\n",
    "# Derived names\n",
    "TABLE_NAME = \"worldbank_indicators\"\n",
    "INDEX_NAME = f\"{TABLE_NAME}_index\"\n",
    "\n",
    "print(f\"Environment: {ENVIRONMENT}\")\n",
    "print(f\"Dry Run: {DRY_RUN}\")\n",
    "print(f\"Catalog (base): {CATALOG_BASE}\")\n",
    "print(f\"Schema: {SCHEMA_NAME}\")\n",
    "print(f\"Endpoint: {ENDPOINT_NAME}\")\n",
    "if MANAGED_LOCATION:\n",
    "    print(f\"Managed Location: {MANAGED_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Add brickkit to path (not yet published to PyPI)\n",
    "# notebookPath() returns workspace-relative path, need /Workspace prefix for filesystem\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_dir = os.path.dirname(notebook_path)\n",
    "# Prepend /Workspace for filesystem access\n",
    "workspace_notebook_dir = f\"/Workspace{notebook_dir}\"\n",
    "src_path = os.path.abspath(os.path.join(workspace_notebook_dir, \"..\", \"..\", \"src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "print(f\"Added to sys.path: {src_path}\")\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# BrickKit imports\n",
    "from brickkit import (\n",
    "    Catalog,\n",
    "    Schema,\n",
    "    Tag,\n",
    "    SecurableType,\n",
    "    VectorSearchEndpoint,\n",
    "    VectorSearchIndex,\n",
    "    load_convention,\n",
    ")\n",
    "from brickkit.models.tables import Table, ColumnInfo\n",
    "from brickkit.models.grants import Principal, AccessPolicy\n",
    "from brickkit.models.principals import ManagedGroup, ManagedServicePrincipal\n",
    "from brickkit.models.enums import PrincipalType, IsolationMode\n",
    "from brickkit.models.workspace_bindings import Workspace, WorkspaceRegistry\n",
    "from brickkit.models.teams import Team\n",
    "from brickkit.executors import (\n",
    "    CatalogExecutor,\n",
    "    SchemaExecutor,\n",
    "    GrantExecutor,\n",
    "    VectorSearchEndpointExecutor,\n",
    "    VectorSearchIndexExecutor,\n",
    "    ServicePrincipalExecutor,\n",
    "    get_privileged_client,\n",
    ")\n",
    "from brickkit.models.base import set_current_environment\n",
    "from brickkit.models.enums import Environment\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set BrickKit environment\n",
    "ENV_MAP = {\"dev\": Environment.DEV, \"acc\": Environment.ACC, \"prd\": Environment.PRD}\n",
    "set_current_environment(ENV_MAP[ENVIRONMENT])\n",
    "\n",
    "print(f\"BrickKit environment set to: {ENVIRONMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD GOVERNANCE CONVENTION ===\n",
    "# The convention defines naming patterns, required tags, and ownership rules\n",
    "\n",
    "# Use absolute path based on notebook location\n",
    "CONVENTION_PATH = os.path.join(workspace_notebook_dir, \"conventions\", \"financial_services.yml\")\n",
    "convention = load_convention(CONVENTION_PATH)\n",
    "\n",
    "print(f\"Loaded convention: {convention.name} (v{convention.version})\")\n",
    "print(f\"Rules: {len(convention.schema.rules)}\")\n",
    "print(f\"Default tags: {len(convention.schema.tags)}\")\n",
    "\n",
    "# Show what the convention enforces\n",
    "for rule in convention.schema.rules:\n",
    "    mode = \"ENFORCED\" if rule.mode.value == \"enforced\" else \"ADVISORY\"\n",
    "    print(f\"  [{mode}] {rule.rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap: Admin Service Principal\n",
    "\n",
    "Notebook tokens have limited permissions (e.g., cannot update catalog isolation mode).\n",
    "We create an \"admin\" service principal with OAuth credentials stored in Databricks Secrets.\n",
    "\n",
    "**Bootstrap flow:**\n",
    "1. Create admin SPN using your user token\n",
    "2. Store SPN credentials in Databricks Secrets\n",
    "3. Grant the admin SPN `MANAGE` + `USE_CATALOG` on the catalog (using your user token)\n",
    "4. Create privileged client from stored credentials\n",
    "5. Use privileged client for catalog operations\n",
    "\n",
    "**First run:** Credentials are created and stored. You need to run the grant cell after defining the catalog.\n",
    "**Subsequent runs:** Credentials loaded from secrets, privileged client ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === BOOTSTRAP: CREATE ADMIN SERVICE PRINCIPAL ===\n# Creates an admin SPN with OAuth credentials for privileged operations.\n# The SPN will be granted catalog access later (after catalog is defined).\n\nADMIN_SPN_NAME = \"spn_brickkit_admin\"\nSECRET_SCOPE = \"brickkit\"\n\n# Define the admin SPN using BrickKit's ManagedServicePrincipal\n# Note: display_name defaults to resolved_name (e.g., spn_brickkit_admin_dev)\nadmin_spn = ManagedServicePrincipal(name=ADMIN_SPN_NAME)\nadmin_spn.add_entitlement(\"workspace-access\")\nadmin_spn.add_entitlement(\"databricks-sql-access\")\n\n# Create a Principal reference for granting permissions\nadmin_spn_principal = Principal(\n    name=ADMIN_SPN_NAME,\n    principal_type=PrincipalType.SERVICE_PRINCIPAL,\n    add_environment_suffix=True,\n)\n\n# Use default client (your user token) for bootstrap operations\nbootstrap_client = WorkspaceClient()\nspn_executor = ServicePrincipalExecutor(bootstrap_client, dry_run=DRY_RUN)\n\n# Check if credentials exist in secrets AND the SPN actually exists in Databricks\nCREDENTIALS_EXIST = False\nADMIN_SPN_APP_ID = None\nSPN_EXISTS = False\n\n# First check if SPN exists in Databricks\ntry:\n    SPN_EXISTS = spn_executor.exists(admin_spn)\n    if SPN_EXISTS:\n        print(f\"✓ SPN {admin_spn.resolved_name} exists in Databricks\")\nexcept Exception as e:\n    print(f\"Could not check SPN existence: {e}\")\n\n# Then check if credentials exist in secrets\ntry:\n    ADMIN_SPN_APP_ID = dbutils.secrets.get(scope=SECRET_SCOPE, key=\"admin-spn-client-id\")\n    print(f\"✓ Credentials found in scope '{SECRET_SCOPE}' (Application ID: {ADMIN_SPN_APP_ID})\")\n    \n    # Only trust credentials if the SPN actually exists\n    if SPN_EXISTS:\n        CREDENTIALS_EXIST = True\n    else:\n        print(f\"⚠ Credentials exist but SPN does not - will recreate SPN and update credentials\")\nexcept Exception:\n    print(f\"No credentials found in scope '{SECRET_SCOPE}'\")\n\n# Create SPN and store credentials if needed\nif not CREDENTIALS_EXIST and not DRY_RUN:\n    print(f\"Creating admin SPN: {admin_spn.resolved_name}\")\n    result, credentials = spn_executor.create_with_secret(admin_spn)\n    print(f\"  {result.operation.value}: {result.message}\")\n    \n    if credentials:\n        # Store credentials in Databricks Secrets (overwrites if they exist)\n        spn_executor.store_credentials(credentials, scope=SECRET_SCOPE)\n        print(f\"  ✓ Stored credentials in scope '{SECRET_SCOPE}'\")\n        ADMIN_SPN_APP_ID = credentials.application_id\n        print(f\"  Application ID: {ADMIN_SPN_APP_ID}\")\n        CREDENTIALS_EXIST = True\n    elif result.operation.value == \"NO_OP\" and SPN_EXISTS:\n        # SPN already exists but we don't have fresh credentials\n        # This shouldn't happen with the logic above, but handle it\n        print(f\"  ⚠ SPN exists but no new credentials generated\")\n        CREDENTIALS_EXIST = False\nelif DRY_RUN:\n    print(f\"[DRY RUN] Would create admin SPN: {admin_spn.resolved_name}\")\n\nprint()\nif CREDENTIALS_EXIST:\n    print(\"✓ Admin SPN ready with valid credentials\")\nelse:\n    print(\"⚠ Admin SPN credentials not available - privileged operations may fail\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Team, Workspace & Principals\n",
    "\n",
    "BrickKit uses `Team` to bring together:\n",
    "- **Workspace** - The Databricks workspace(s) per environment\n",
    "- **Principals** - Service principals and groups that own/access resources\n",
    "- **Catalog bindings** - Automatically configured via `team.add_catalog()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEFINE WORKSPACE ===\n",
    "# Register the workspace. In Free Edition with \"Default Storage\", catalogs are workspace-bound.\n",
    "\n",
    "WORKSPACE_ID = \"4188055811360976\"  # Your workspace ID\n",
    "WORKSPACE_HOSTNAME = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\n",
    "\n",
    "# Register workspace in BrickKit's global registry\n",
    "registry = WorkspaceRegistry()\n",
    "dev_workspace = registry.get_or_create(\n",
    "    workspace_id=WORKSPACE_ID, name=\"free-edition-workspace\", hostname=WORKSPACE_HOSTNAME, environment=Environment.DEV\n",
    ")\n",
    "\n",
    "print(f\"Workspace: {dev_workspace.name}\")\n",
    "print(f\"  ID: {dev_workspace.workspace_id}\")\n",
    "print(f\"  Environment: {dev_workspace.environment.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === DEFINE PRINCIPALS ===\n# Service principals and groups that will own/access resources.\n# These are environment-aware: names automatically get _dev/_acc/_prd suffixes.\n\n# Service Principal for catalog ownership (convention requirement)\n# Note: display_name defaults to resolved_name (e.g., spn_trading_platform_dev)\ntrading_platform_spn = ManagedServicePrincipal(name=\"spn_trading_platform\")\ntrading_platform_spn.add_entitlement(\"workspace-access\")\ntrading_platform_spn.add_entitlement(\"databricks-sql-access\")\n\n# Group for schema ownership and team access\n# Note: display_name defaults to resolved_name (e.g., grp_quant_team_dev)\nquant_team_group = ManagedGroup(name=\"grp_quant_team\")\nquant_team_group.add_entitlement(\"workspace-access\")\nquant_team_group.add_entitlement(\"databricks-sql-access\")\n\n# NOTE: Members can be added after creation using group_executor.sync_members()\n# For this demo, we create the group without members.\n# In production, add real users that exist in your workspace:\n#   quant_team_group.add_user(\"real.user@yourcompany.com\")\n#   quant_team_group.add_service_principal(\"spn_trading_platform\")\n\nprint(f\"Service Principal: {trading_platform_spn.resolved_name}\")\nprint(f\"Group: {quant_team_group.resolved_name}\")"
  },
  {
   "cell_type": "code",
   "source": "# === DEPLOY TEAM PRINCIPALS ===\n# Create the service principal and group in Databricks before using them as owners.\n# This is separate from the admin SPN (which is for privileged operations).\n# The executors are idempotent - they return NO_OP if the principal already exists.\n\nfrom brickkit.executors import GroupExecutor\n\ngroup_executor = GroupExecutor(bootstrap_client, dry_run=DRY_RUN)\n\n# Deploy trading platform SPN (will be catalog owner)\nprint(f\"Deploying service principal: {trading_platform_spn.resolved_name}\")\nresult = spn_executor.create(trading_platform_spn)\nprint(f\"  {result.operation.value}: {result.message}\")\n\n# Deploy quant team group (will be schema owner)\nprint(f\"Deploying group: {quant_team_group.resolved_name}\")\nresult = group_executor.create(quant_team_group)\nprint(f\"  {result.operation.value}: {result.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEFINE TEAM ===\n",
    "# Team brings together workspace, principals, and manages catalog bindings.\n",
    "\n",
    "# Convert ManagedServicePrincipal/ManagedGroup to Principal for team membership\n",
    "catalog_owner = Principal(\n",
    "    name=trading_platform_spn.name,\n",
    "    principal_type=PrincipalType.SERVICE_PRINCIPAL,\n",
    "    add_environment_suffix=trading_platform_spn.add_environment_suffix,\n",
    ")\n",
    "\n",
    "schema_owner = Principal(\n",
    "    name=quant_team_group.name,\n",
    "    principal_type=PrincipalType.GROUP,\n",
    "    add_environment_suffix=quant_team_group.add_environment_suffix,\n",
    ")\n",
    "\n",
    "# Create team and add workspace + principals\n",
    "quant_team = Team(name=\"quant_trading\")\n",
    "quant_team.add_workspace(dev_workspace)\n",
    "quant_team.add_principal(catalog_owner)\n",
    "quant_team.add_principal(schema_owner)\n",
    "\n",
    "print(f\"Team: {quant_team.name}\")\n",
    "print(f\"  Workspaces: {list(quant_team.workspaces.keys())}\")\n",
    "print(f\"  Principals: {[p.resolved_name for p in quant_team.principals]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Sample Data\n",
    "\n",
    "We'll use a small inline dataset of World Bank indicators. This lets you run the full demo quickly without external API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAMPLE DATA ===\n",
    "# 20 World Bank indicators with embedding text for vector search\n",
    "\n",
    "SAMPLE_INDICATORS = [\n",
    "    (\n",
    "        \"SP.POP.TOTL\",\n",
    "        \"Population, total\",\n",
    "        \"Total population counts all residents regardless of legal status or citizenship.\",\n",
    "        \"Demographics\",\n",
    "    ),\n",
    "    (\n",
    "        \"NY.GDP.MKTP.CD\",\n",
    "        \"GDP (current US$)\",\n",
    "        \"GDP at purchaser's prices is the sum of gross value added by all resident producers.\",\n",
    "        \"Economy\",\n",
    "    ),\n",
    "    (\n",
    "        \"NY.GDP.PCAP.CD\",\n",
    "        \"GDP per capita (current US$)\",\n",
    "        \"GDP per capita is gross domestic product divided by midyear population.\",\n",
    "        \"Economy\",\n",
    "    ),\n",
    "    (\n",
    "        \"SI.POV.DDAY\",\n",
    "        \"Poverty headcount ratio at $2.15 a day\",\n",
    "        \"Poverty headcount ratio at $2.15 a day is the percentage of the population living on less than $2.15 a day.\",\n",
    "        \"Poverty\",\n",
    "    ),\n",
    "    (\n",
    "        \"SI.POV.GINI\",\n",
    "        \"Gini index\",\n",
    "        \"Gini index measures the extent to which the distribution of income among individuals deviates from a perfectly equal distribution.\",\n",
    "        \"Inequality\",\n",
    "    ),\n",
    "    (\n",
    "        \"SL.UEM.TOTL.ZS\",\n",
    "        \"Unemployment, total (% of labor force)\",\n",
    "        \"Unemployment refers to the share of the labor force that is without work but available and seeking employment.\",\n",
    "        \"Labor\",\n",
    "    ),\n",
    "    (\n",
    "        \"FP.CPI.TOTL.ZG\",\n",
    "        \"Inflation, consumer prices (annual %)\",\n",
    "        \"Inflation as measured by the consumer price index reflects the annual percentage change in the cost of goods and services.\",\n",
    "        \"Economy\",\n",
    "    ),\n",
    "    (\n",
    "        \"SP.DYN.LE00.IN\",\n",
    "        \"Life expectancy at birth, total (years)\",\n",
    "        \"Life expectancy at birth indicates the number of years a newborn infant would live if patterns of mortality at birth were to stay the same.\",\n",
    "        \"Health\",\n",
    "    ),\n",
    "    (\n",
    "        \"SH.DYN.MORT\",\n",
    "        \"Mortality rate, under-5 (per 1,000 live births)\",\n",
    "        \"Under-five mortality rate is the probability per 1,000 that a newborn baby will die before reaching age five.\",\n",
    "        \"Health\",\n",
    "    ),\n",
    "    (\n",
    "        \"SE.ADT.LITR.ZS\",\n",
    "        \"Literacy rate, adult total (% of people ages 15 and above)\",\n",
    "        \"Adult literacy rate is the percentage of people ages 15 and above who can read and write a short simple statement.\",\n",
    "        \"Education\",\n",
    "    ),\n",
    "    (\n",
    "        \"SE.PRM.ENRR\",\n",
    "        \"School enrollment, primary (% gross)\",\n",
    "        \"Gross enrollment ratio is the ratio of total enrollment to the population of the age group that officially corresponds to the level of education.\",\n",
    "        \"Education\",\n",
    "    ),\n",
    "    (\n",
    "        \"EG.USE.ELEC.KH.PC\",\n",
    "        \"Electric power consumption (kWh per capita)\",\n",
    "        \"Electric power consumption measures the production of power plants and combined heat and power plants less transmission losses.\",\n",
    "        \"Energy\",\n",
    "    ),\n",
    "    (\n",
    "        \"EN.ATM.CO2E.PC\",\n",
    "        \"CO2 emissions (metric tons per capita)\",\n",
    "        \"Carbon dioxide emissions are those stemming from the burning of fossil fuels and the manufacture of cement.\",\n",
    "        \"Environment\",\n",
    "    ),\n",
    "    (\n",
    "        \"AG.LND.FRST.ZS\",\n",
    "        \"Forest area (% of land area)\",\n",
    "        \"Forest area is land under natural or planted stands of trees of at least 5 meters in situ.\",\n",
    "        \"Environment\",\n",
    "    ),\n",
    "    (\n",
    "        \"SH.XPD.CHEX.PC.CD\",\n",
    "        \"Current health expenditure per capita (current US$)\",\n",
    "        \"Current expenditures on health per capita in current US dollars.\",\n",
    "        \"Health\",\n",
    "    ),\n",
    "    (\n",
    "        \"IT.NET.USER.ZS\",\n",
    "        \"Individuals using the Internet (% of population)\",\n",
    "        \"Internet users are individuals who have used the Internet in the last 3 months.\",\n",
    "        \"Technology\",\n",
    "    ),\n",
    "    (\n",
    "        \"BX.KLT.DINV.CD.WD\",\n",
    "        \"Foreign direct investment, net inflows (BoP, current US$)\",\n",
    "        \"Foreign direct investment are the net inflows of investment to acquire a lasting management interest.\",\n",
    "        \"Economy\",\n",
    "    ),\n",
    "    (\n",
    "        \"GC.DOD.TOTL.GD.ZS\",\n",
    "        \"Central government debt, total (% of GDP)\",\n",
    "        \"Debt is the entire stock of direct government fixed-term contractual obligations to others outstanding.\",\n",
    "        \"Economy\",\n",
    "    ),\n",
    "    (\n",
    "        \"NE.EXP.GNFS.ZS\",\n",
    "        \"Exports of goods and services (% of GDP)\",\n",
    "        \"Exports of goods and services represent the value of all goods and other market services provided to the rest of the world.\",\n",
    "        \"Trade\",\n",
    "    ),\n",
    "    (\n",
    "        \"NE.IMP.GNFS.ZS\",\n",
    "        \"Imports of goods and services (% of GDP)\",\n",
    "        \"Imports of goods and services represent the value of all goods and other market services received from the rest of the world.\",\n",
    "        \"Trade\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Schema for the indicators table\n",
    "INDICATORS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"indicator_id\", StringType(), False),\n",
    "        StructField(\"indicator_name\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"topic\", StringType(), True),\n",
    "        StructField(\"embedding_text\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def create_sample_dataframe(spark: SparkSession):\n",
    "    \"\"\"Create DataFrame from sample indicators with embedding text.\"\"\"\n",
    "    rows = [(ind_id, name, desc, topic, f\"{name}. {desc}\") for ind_id, name, desc, topic in SAMPLE_INDICATORS]\n",
    "    return spark.createDataFrame(rows, INDICATORS_SCHEMA)\n",
    "\n",
    "\n",
    "# Preview the sample data\n",
    "sample_df = create_sample_dataframe(spark)\n",
    "print(f\"Sample data: {sample_df.count()} indicators\")\n",
    "sample_df.select(\"indicator_id\", \"indicator_name\", \"topic\").show(5, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Fetch Real Data from World Bank API\n",
    "\n",
    "Uncomment and run the cell below to fetch real indicator metadata. This takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTIONAL: FETCH FROM WORLD BANK API ===\n",
    "# Uncomment this cell to fetch real data (takes several minutes)\n",
    "\n",
    "# %pip install wbgapi requests tqdm --quiet\n",
    "\n",
    "# import wbgapi as wb\n",
    "# import requests\n",
    "# from requests.exceptions import RequestException, Timeout\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def fetch_worldbank_indicators(spark: SparkSession, limit: int = 100):\n",
    "#     \"\"\"Fetch indicator metadata from World Bank API.\"\"\"\n",
    "#     series_list = wb.series.info()\n",
    "#     series_ids = [s.get(\"id\") for s in series_list.items][:limit]\n",
    "#\n",
    "#     rows = []\n",
    "#     for series_id in tqdm(series_ids, desc=\"Fetching\"):\n",
    "#         try:\n",
    "#             url = f\"https://api.worldbank.org/v2/indicator/{series_id}?format=json\"\n",
    "#             resp = requests.get(url, timeout=30)\n",
    "#             resp.raise_for_status()\n",
    "#             data = resp.json()\n",
    "#             if len(data) >= 2 and data[1]:\n",
    "#                 meta = data[1][0]\n",
    "#                 name = meta.get(\"name\", \"\") or \"\"\n",
    "#                 desc = meta.get(\"sourceNote\", \"\") or \"\"\n",
    "#                 topics = meta.get(\"topics\", []) or []\n",
    "#                 topic = topics[0].get(\"value\", \"\") if topics else \"\"\n",
    "#                 embedding_text = f\"{name}. {desc}\".strip()\n",
    "#                 rows.append((series_id, name, desc, topic, embedding_text))\n",
    "#         except (RequestException, Timeout, ValueError) as e:\n",
    "#             print(f\"Skipping {series_id}: {e}\")\n",
    "#\n",
    "#     return spark.createDataFrame(rows, INDICATORS_SCHEMA)\n",
    "\n",
    "# # Fetch real data (uncomment to use)\n",
    "# sample_df = fetch_worldbank_indicators(spark, limit=500)\n",
    "# print(f\"Fetched {sample_df.count()} indicators from World Bank API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Governed Resources\n",
    "\n",
    "Now we define our resources using BrickKit models. The convention automatically applies:\n",
    "- Environment-specific naming (e.g., `quant_risk_dev`)\n",
    "- Required governance tags\n",
    "- Ownership rules validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ENVIRONMENT SETUP ===\n",
    "# catalog_owner and schema_owner are already defined in the Team cell above\n",
    "\n",
    "environment = ENV_MAP[ENVIRONMENT]\n",
    "\n",
    "print(f\"Catalog owner: {catalog_owner.resolved_name} ({catalog_owner.principal_type.value})\")\n",
    "print(f\"Schema owner: {schema_owner.resolved_name} ({schema_owner.principal_type.value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CATALOG ===\n",
    "# NOTE: In Free Edition workspaces with \"Default Storage\", catalogs cannot be created via SDK.\n",
    "# Create the catalog manually via UI first (with \"Use default storage\" checked), then reference it here.\n",
    "\n",
    "catalog_name = convention.generate_name(SecurableType.CATALOG, environment)\n",
    "\n",
    "catalog = Catalog(\n",
    "    name=catalog_name,\n",
    "    owner=catalog_owner,\n",
    "    comment=\"Risk Analytics catalog for quantitative trading\",\n",
    "    isolation_mode=IsolationMode.ISOLATED,  # Default Storage catalogs are workspace-isolated\n",
    ")\n",
    "\n",
    "# Use Team to automatically configure workspace bindings\n",
    "# This sets catalog.workspace_ids based on the team's workspace for this environment\n",
    "quant_team.add_catalog(catalog)\n",
    "\n",
    "# Apply convention (adds tags, validates rules)\n",
    "convention.apply_to(catalog, environment)\n",
    "errors = convention.get_validation_errors(catalog)\n",
    "if errors:\n",
    "    raise ValueError(f\"Catalog validation failed: {errors}\")\n",
    "\n",
    "print(f\"Catalog: {catalog.name}\")\n",
    "print(f\"  Isolation Mode: {catalog.isolation_mode.value}\")\n",
    "print(f\"  Workspace IDs: {catalog.workspace_ids} (auto-configured by Team)\")\n",
    "print(f\"  Tags: {len(catalog.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SCHEMA ===\n",
    "schema = Schema(\n",
    "    name=SCHEMA_NAME,\n",
    "    catalog_name=catalog.name,\n",
    "    owner=schema_owner,\n",
    "    comment=\"World Bank indicator metadata for vector search\",\n",
    ")\n",
    "\n",
    "convention.apply_to(schema, environment)\n",
    "errors = convention.get_validation_errors(schema)\n",
    "if errors:\n",
    "    raise ValueError(f\"Schema validation failed: {errors}\")\n",
    "\n",
    "print(f\"Schema: {schema.fqdn}\")\n",
    "print(f\"  Tags: {len(schema.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TABLE ===\n",
    "# Define the table structure with BrickKit (not just raw PySpark write)\n",
    "\n",
    "table = Table(\n",
    "    name=TABLE_NAME,\n",
    "    catalog_name=catalog.name,\n",
    "    schema_name=schema.name,\n",
    "    owner=schema_owner,\n",
    "    comment=\"World Bank indicator metadata with embeddings for semantic search\",\n",
    "    columns=[\n",
    "        ColumnInfo(name=\"indicator_id\", type=\"STRING\", nullable=False, comment=\"World Bank indicator code\"),\n",
    "        ColumnInfo(name=\"indicator_name\", type=\"STRING\", nullable=True, comment=\"Human-readable indicator name\"),\n",
    "        ColumnInfo(name=\"description\", type=\"STRING\", nullable=True, comment=\"Full description of the indicator\"),\n",
    "        ColumnInfo(name=\"topic\", type=\"STRING\", nullable=True, comment=\"Category/topic of the indicator\"),\n",
    "        ColumnInfo(name=\"embedding_text\", type=\"STRING\", nullable=True, comment=\"Text used for embedding generation\"),\n",
    "    ],\n",
    "    tags=[\n",
    "        Tag(key=\"data_source\", value=\"worldbank_api\"),\n",
    "        Tag(key=\"refresh_frequency\", value=\"weekly\"),\n",
    "        Tag(key=\"contains_pii\", value=\"false\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "convention.apply_to(table, environment)\n",
    "errors = convention.get_validation_errors(table)\n",
    "if errors:\n",
    "    raise ValueError(f\"Table validation failed: {errors}\")\n",
    "\n",
    "print(f\"Table: {table.fqdn}\")\n",
    "print(f\"  Columns: {len(table.columns)}\")\n",
    "print(f\"  Tags: {len(table.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VECTOR SEARCH ENDPOINT ===\n",
    "vs_endpoint = VectorSearchEndpoint(\n",
    "    name=ENDPOINT_NAME,\n",
    "    comment=\"Semantic search endpoint for risk analytics indicators\",\n",
    "    tags=[\n",
    "        Tag(key=\"purpose\", value=\"semantic_search\"),\n",
    "        Tag(key=\"model\", value=\"databricks-bge-large-en\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "convention.apply_to(vs_endpoint, environment)\n",
    "errors = convention.get_validation_errors(vs_endpoint)\n",
    "if errors:\n",
    "    raise ValueError(f\"Endpoint validation failed: {errors}\")\n",
    "\n",
    "print(f\"Endpoint: {vs_endpoint.resolved_name}\")\n",
    "print(f\"  Tags: {len(vs_endpoint.tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VECTOR SEARCH INDEX ===\n",
    "# Use table.fqdn to reference the governed table\n",
    "\n",
    "vs_index = VectorSearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    source_table=table.fqdn,  # Reference the governed Table model\n",
    "    primary_key=\"indicator_id\",\n",
    "    embedding_column=\"embedding_text\",\n",
    "    embedding_model=\"databricks-bge-large-en\",\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    tags=[\n",
    "        Tag(key=\"index_type\", value=\"managed_embedding\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "convention.apply_to(vs_index, environment)\n",
    "errors = convention.get_validation_errors(vs_index)\n",
    "if errors:\n",
    "    raise ValueError(f\"Index validation failed: {errors}\")\n",
    "\n",
    "print(f\"Index: {vs_index.resolved_name}\")\n",
    "print(f\"  Source: {vs_index.source_table}\")\n",
    "print(f\"  Endpoint: {vs_index.resolved_endpoint_name}\")\n",
    "print(f\"  Tags: {len(vs_index.tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deploy with BrickKit Executors\n",
    "\n",
    "BrickKit executors handle:\n",
    "- Idempotent create (skip if exists)\n",
    "- Wait for provisioning\n",
    "- Tag application\n",
    "- **Permission grants** - Ensure teams have access after ownership change\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === INITIALIZE CLIENTS AND EXECUTORS ===\n\n# Default client for most operations (uses your notebook token)\nws_client = WorkspaceClient()\nvs_client = VectorSearchClient()\n\n# Grant admin SPN access to the catalog FIRST (using SQL via your user token)\n# This is required before the admin SPN can manage the catalog\nif CREDENTIALS_EXIST and not DRY_RUN:\n    print(f\"Granting admin SPN ({admin_spn.resolved_name}) access to catalog {catalog.resolved_name}...\")\n    \n    # Use SQL grants - simpler and more reliable for bootstrap\n    grant_sql = f\"\"\"\n        GRANT MANAGE, USE CATALOG ON CATALOG `{catalog.resolved_name}` \n        TO `{ADMIN_SPN_APP_ID}`\n    \"\"\"\n    try:\n        spark.sql(grant_sql)\n        print(f\"  ✓ Granted MANAGE + USE CATALOG to {admin_spn.resolved_name}\")\n    except Exception as e:\n        if \"already has\" in str(e).lower() or \"already granted\" in str(e).lower():\n            print(f\"  ✓ Privileges already granted\")\n        else:\n            raise\n\n# Create privileged client from stored credentials\nPRIVILEGED_CLIENT = None\nif CREDENTIALS_EXIST:\n    PRIVILEGED_CLIENT = get_privileged_client(\n        host=WORKSPACE_HOSTNAME,\n        scope=SECRET_SCOPE,\n        dbutils=dbutils,\n    )\n    print(f\"✓ Privileged client ready (using {admin_spn.resolved_name})\")\n\n# Use privileged client for catalog operations, default for others\ncatalog_client = PRIVILEGED_CLIENT if PRIVILEGED_CLIENT else ws_client\nif PRIVILEGED_CLIENT:\n    print(\"Using privileged client (SPN) for catalog operations\")\nelse:\n    print(\"⚠ No privileged client - catalog updates may fail due to token restrictions\")\n\n# Initialize executors\ncatalog_executor = CatalogExecutor(catalog_client, dry_run=DRY_RUN)  # Uses privileged client\nschema_executor = SchemaExecutor(ws_client, dry_run=DRY_RUN)\ngrant_executor = GrantExecutor(ws_client, dry_run=DRY_RUN)\nendpoint_executor = VectorSearchEndpointExecutor(ws_client, dry_run=DRY_RUN)\nindex_executor = VectorSearchIndexExecutor(ws_client, dry_run=DRY_RUN)\n\nprint(f\"Executors initialized (dry_run={DRY_RUN})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY CATALOG ===\n",
    "# In Free Edition with \"Default Storage\", create the catalog manually via UI first.\n",
    "# The executor handles existing catalogs gracefully (returns NO_OP if already exists).\n",
    "result = catalog_executor.create(catalog)\n",
    "print(f\"Catalog: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY SCHEMA ===\n",
    "result = schema_executor.create(schema)\n",
    "print(f\"Schema: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === APPLY GRANTS ===\n",
    "# After ownership change, the deploying user loses access. Grant permissions to the team group.\n",
    "# This ensures the quant_team_group can access the catalog and work with the data.\n",
    "\n",
    "# Grant the team group access to the catalog\n",
    "# The group (grp_quant_team) needs USE_CATALOG + CREATE_SCHEMA to work in the catalog\n",
    "catalog.grant(schema_owner, AccessPolicy.WRITER())\n",
    "\n",
    "# Apply all accumulated grants to Databricks\n",
    "results = grant_executor.apply_privileges(catalog.privileges)\n",
    "for result in results:\n",
    "    print(f\"Grants on {result.resource_name}: {result.operation.value} - {result.message}\")\n",
    "\n",
    "# Show what was granted\n",
    "print(f\"\\nPrivileges granted on catalog to {quant_team_group.resolved_name}:\")\n",
    "for priv in catalog.privileges:\n",
    "    print(f\"  - {priv.privilege.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WRITE DATA TO TABLE ===\n",
    "# Using the governed Table model's fqdn\n",
    "\n",
    "if not DRY_RUN:\n",
    "    # Write with Delta format and Change Data Feed enabled\n",
    "    (\n",
    "        sample_df.write.format(\"delta\")\n",
    "        .option(\"delta.enableChangeDataFeed\", \"true\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(table.fqdn)\n",
    "    )\n",
    "\n",
    "    # Verify\n",
    "    count = spark.table(table.fqdn).count()\n",
    "    print(f\"Table: {table.fqdn} - {count} rows written\")\n",
    "else:\n",
    "    print(f\"[DRY RUN] Would write {sample_df.count()} rows to {table.fqdn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY VECTOR SEARCH ENDPOINT ===\n",
    "result = endpoint_executor.create(vs_endpoint)\n",
    "print(f\"Endpoint: {result.operation.value} - {result.message}\")\n",
    "\n",
    "# Wait for endpoint to be online (uses executor's built-in wait logic)\n",
    "if not DRY_RUN and result.operation.value == \"CREATE\":\n",
    "    print(\"Waiting for endpoint to be online...\")\n",
    "    if endpoint_executor.wait_for_endpoint(vs_endpoint):\n",
    "        print(f\"Endpoint {vs_endpoint.resolved_name} is ONLINE\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Endpoint {vs_endpoint.resolved_name} failed to provision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOY VECTOR SEARCH INDEX ===\n",
    "result = index_executor.create(vs_index)\n",
    "print(f\"Index: {result.operation.value} - {result.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test Vector Search\n",
    "\n",
    "The index syncs asynchronously. Once ready, we can run similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECK INDEX STATUS ===\n",
    "\n",
    "if not DRY_RUN:\n",
    "    FULL_INDEX_NAME = f\"{catalog.name}.{schema.name}.{vs_index.resolved_name}\"\n",
    "\n",
    "    index = vs_client.get_index(\n",
    "        endpoint_name=vs_endpoint.resolved_name,\n",
    "        index_name=FULL_INDEX_NAME,\n",
    "    )\n",
    "    status = index.describe().get(\"status\", {})\n",
    "    print(f\"Index status: ready={status.get('ready', 'UNKNOWN')}\")\n",
    "    print(f\"Message: {status.get('message', 'N/A')}\")\n",
    "else:\n",
    "    print(\"[DRY RUN] Would check index status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN SIMILARITY SEARCH ===\n",
    "\n",
    "if not DRY_RUN:\n",
    "    TEST_QUERY = \"poverty and inequality measures\"\n",
    "\n",
    "    try:\n",
    "        results = index.similarity_search(\n",
    "            query_text=TEST_QUERY,\n",
    "            columns=[\"indicator_id\", \"indicator_name\", \"description\", \"topic\"],\n",
    "            num_results=5,\n",
    "        )\n",
    "\n",
    "        print(f\"Search: '{TEST_QUERY}'\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        data = results.get(\"result\", {}).get(\"data_array\", [])\n",
    "        for i, row in enumerate(data, 1):\n",
    "            print(f\"{i}. [{row[3]}] {row[1]}\")\n",
    "            print(f\"   {row[2][:80]}...\")\n",
    "            print()\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"not ready\" in str(e).lower() or \"syncing\" in str(e).lower():\n",
    "            print(\"Index is still syncing. Please wait and try again.\")\n",
    "        else:\n",
    "            raise\n",
    "else:\n",
    "    print(\"[DRY RUN] Would run similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. What BrickKit Added (Governance Value)\n",
    "\n",
    "Let's see what governance BrickKit applied automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GOVERNANCE SUMMARY ===\n",
    "\n",
    "\n",
    "def display_resource_governance(name: str, resource):\n",
    "    \"\"\"Display governance metadata for a resource.\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Name (with environment suffix)\n",
    "    if hasattr(resource, \"resolved_name\"):\n",
    "        print(f\"Name: {resource.resolved_name}\")\n",
    "    elif hasattr(resource, \"fqdn\"):\n",
    "        try:\n",
    "            print(f\"Name: {resource.fqdn}\")\n",
    "        except ValueError:\n",
    "            print(f\"Name: {resource.name}\")\n",
    "    else:\n",
    "        print(f\"Name: {resource.name}\")\n",
    "\n",
    "    # Owner\n",
    "    if hasattr(resource, \"owner\") and resource.owner:\n",
    "        owner = resource.owner\n",
    "        print(f\"Owner: {owner.resolved_name} ({owner.principal_type.value})\")\n",
    "\n",
    "    # Isolation mode and workspace bindings (for catalogs)\n",
    "    if hasattr(resource, \"isolation_mode\") and resource.isolation_mode:\n",
    "        print(f\"Isolation Mode: {resource.isolation_mode.value}\")\n",
    "    if hasattr(resource, \"workspace_ids\") and resource.workspace_ids:\n",
    "        print(f\"Workspace IDs: {resource.workspace_ids}\")\n",
    "\n",
    "    # Privileges (grants)\n",
    "    if hasattr(resource, \"privileges\") and resource.privileges:\n",
    "        print(f\"Privileges ({len(resource.privileges)}):\")\n",
    "        for priv in resource.privileges:\n",
    "            print(f\"  - {priv.principal}: {priv.privilege.value}\")\n",
    "\n",
    "    # Request for Access (RFA)\n",
    "    if hasattr(resource, \"request_for_access\") and resource.request_for_access:\n",
    "        rfa = resource.request_for_access\n",
    "        print(f\"RFA Destination: {rfa.destination}\")\n",
    "        if rfa.instructions:\n",
    "            print(f\"RFA Instructions: {rfa.instructions}\")\n",
    "\n",
    "    # Columns (for tables)\n",
    "    if hasattr(resource, \"columns\") and resource.columns:\n",
    "        print(f\"Columns: {len(resource.columns)}\")\n",
    "\n",
    "    # Tags\n",
    "    if hasattr(resource, \"tags\") and resource.tags:\n",
    "        print(f\"Tags ({len(resource.tags)}):\")\n",
    "        for tag in sorted(resource.tags, key=lambda t: t.key):\n",
    "            print(f\"  - {tag.key}: {tag.value}\")\n",
    "\n",
    "\n",
    "# Display Team\n",
    "print(\"=\" * 60)\n",
    "print(\"TEAM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Name: {quant_team.name}\")\n",
    "print(f\"  Workspaces:\")\n",
    "for env, ws in quant_team.workspaces.items():\n",
    "    print(f\"    - {env.value}: {ws.name} (ID: {ws.workspace_id})\")\n",
    "print(f\"  Principals:\")\n",
    "for p in quant_team.principals:\n",
    "    print(f\"    - {p.resolved_name} ({p.principal_type.value})\")\n",
    "\n",
    "# Display defined principals (detailed)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRINCIPALS (DETAILED)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nService Principal: {trading_platform_spn.resolved_name}\")\n",
    "print(f\"  Display Name: {trading_platform_spn.display_name}\")\n",
    "print(f\"  Entitlements: {trading_platform_spn.entitlements}\")\n",
    "\n",
    "print(f\"\\nGroup: {quant_team_group.resolved_name}\")\n",
    "print(f\"  Display Name: {quant_team_group.display_name}\")\n",
    "print(f\"  Entitlements: {quant_team_group.entitlements}\")\n",
    "print(f\"  Members ({len(quant_team_group.members)}):\")\n",
    "for member in quant_team_group.members:\n",
    "    print(f\"    - {member.resolved_name} ({member.principal_type.value})\")\n",
    "\n",
    "# Display governance for all resources\n",
    "display_resource_governance(\"CATALOG\", catalog)\n",
    "display_resource_governance(\"SCHEMA\", schema)\n",
    "display_resource_governance(\"TABLE\", table)\n",
    "display_resource_governance(\"VECTOR SEARCH ENDPOINT\", vs_endpoint)\n",
    "display_resource_governance(\"VECTOR SEARCH INDEX\", vs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONVENTION RULES APPLIED ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONVENTION RULES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Convention: {convention.name} (v{convention.version})\")\n",
    "print()\n",
    "\n",
    "for rule in convention.schema.rules:\n",
    "    mode = \"ENFORCED\" if rule.mode.value == \"enforced\" else \"ADVISORY\"\n",
    "    print(f\"[{mode}] {rule.rule}\")\n",
    "\n",
    "print()\n",
    "print(\"What this means:\")\n",
    "print(\"- Catalogs MUST be owned by service principals (not users)\")\n",
    "print(\"- All resources MUST be owned by SP or Group (no individual users)\")\n",
    "print(\"- Resources SHOULD have cost_center and team tags\")\n",
    "print(\"- BrickKit validated all these rules before deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WHAT YOU DIDN'T HAVE TO DO ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHAT BRICKKIT DID FOR YOU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "benefits = [\n",
    "    (\"Team definition\", f\"Team '{quant_team.name}' brings together workspace + principals\"),\n",
    "    (\"Principal definitions\", f\"Defined SP + Group with {len(quant_team_group.members)} members declaratively\"),\n",
    "    (\"Workspace binding\", f\"team.add_catalog() auto-configured workspace IDs: {catalog.workspace_ids}\"),\n",
    "    (\"Environment suffixes\", f\"All names automatically suffixed with '_{ENVIRONMENT}'\"),\n",
    "    (\"Governance tags\", f\"{len(catalog.tags)} tags auto-applied from convention\"),\n",
    "    (\"Ownership validation\", \"Verified catalog has SP owner, schema has Group owner\"),\n",
    "    (\"Permission grants\", f\"Granted {len(catalog.privileges)} privileges to team group after ownership change\"),\n",
    "    (\"Request for Access\", \"RFA configured with inheritance (table inherits from schema)\"),\n",
    "    (\"Idempotent deployment\", \"Executors skip if resource exists, sync tags if needed\"),\n",
    "    (\"Wait logic\", \"Built-in endpoint provisioning wait with timeout/retry\"),\n",
    "    (\"Consistent patterns\", \"Same governance across Catalog, Schema, Endpoint, Index\"),\n",
    "]\n",
    "\n",
    "for benefit, detail in benefits:\n",
    "    print(f\"\\n{benefit}:\")\n",
    "    print(f\"  {detail}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Without BrickKit, you would manually:\")\n",
    "print(\"  - Define groups/SPNs with raw SDK calls\")\n",
    "print(\"  - Track which workspace IDs to bind to each catalog\")\n",
    "print(\"  - Add environment suffixes to every resource name\")\n",
    "print(\"  - Remember which tags to apply (and apply them consistently)\")\n",
    "print(\"  - Validate ownership rules before deployment\")\n",
    "print(\"  - Grant permissions to teams after changing ownership\")\n",
    "print(\"  - Configure RFA on each securable individually\")\n",
    "print(\"  - Write wait/retry logic for endpoint provisioning\")\n",
    "print(\"  - Handle idempotency (check exists, update tags, etc.)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This demo showed:\n",
    "\n",
    "1. **Convention Loading** - Governance rules from YAML\n",
    "2. **Team Definition** - `Team` with `Workspace` and `Principal` members\n",
    "3. **Principal Definition** - `ManagedServicePrincipal`, `ManagedGroup` with members\n",
    "4. **Catalog Binding** - `team.add_catalog()` auto-configures workspace IDs\n",
    "5. **Governed Models** - `Catalog`, `Schema`, `Table`, `VectorSearchEndpoint`, `VectorSearchIndex`\n",
    "6. **Executors** - Idempotent deployment with built-in wait logic\n",
    "7. **Permission Grants** - Ensure team access after ownership changes\n",
    "8. **Automatic Governance** - Tags, naming, ownership validation\n",
    "\n",
    "### BrickKit vs DAB Recap\n",
    "\n",
    "- **DAB** handles: notebook sync, job definitions, workflow orchestration\n",
    "- **BrickKit** handles: teams, principals, catalog/schema/table creation, workspace bindings, VS endpoint/index, grants, tags, validation\n",
    "- **Together**: DAB runs this notebook as a job, BrickKit deploys the governed resources\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Modify `conventions/financial_services.yml` to change governance rules\n",
    "- Set `dry_run=false` to deploy for real\n",
    "- Try different environments (`dev`, `acc`, `prd`) to see naming changes\n",
    "- Add more workspaces to the team for multi-environment deployments\n",
    "- Add your own data source instead of sample indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}